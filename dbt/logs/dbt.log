[0m14:52:59.819455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71585c7958d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71585cb1cc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71585c795bd0>]}


============================== 14:52:59.821569 | e1f24119-634e-446c-b2bc-5ecf5e69f13f ==============================
[0m14:52:59.821569 [info ] [MainThread]: Running with dbt=1.8.5
[0m14:52:59.822068 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:52:59.829376 [info ] [MainThread]: dbt version: 1.8.5
[0m14:52:59.829834 [info ] [MainThread]: python version: 3.11.5
[0m14:52:59.830286 [info ] [MainThread]: python path: /home/micasidad/Desktop/yt-comment-analysis/venv/bin/python
[0m14:52:59.830672 [info ] [MainThread]: os info: Linux-6.5.0-44-generic-x86_64-with-glibc2.35
[0m14:53:00.654581 [info ] [MainThread]: Using profiles dir at /home/micasidad/.dbt
[0m14:53:00.655218 [info ] [MainThread]: Using profiles.yml file at /home/micasidad/.dbt/profiles.yml
[0m14:53:00.655705 [info ] [MainThread]: Using dbt_project.yml file at /home/micasidad/Desktop/yt-comment-analysis/dbt/dbt_project.yml
[0m14:53:00.656129 [info ] [MainThread]: adapter type: bigquery
[0m14:53:00.656544 [info ] [MainThread]: adapter version: 1.8.2
[0m14:53:00.725186 [info ] [MainThread]: Configuration:
[0m14:53:00.725684 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:53:00.726160 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:53:00.726534 [info ] [MainThread]: Required dependencies:
[0m14:53:00.726926 [debug] [MainThread]: Executing "git --help"
[0m14:53:00.728963 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:53:00.729341 [debug] [MainThread]: STDERR: "b''"
[0m14:53:00.729625 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:53:00.729991 [info ] [MainThread]: Connection:
[0m14:53:00.730394 [info ] [MainThread]:   method: service-account
[0m14:53:00.730835 [info ] [MainThread]:   database: sentiment-analysis-410608
[0m14:53:00.731213 [info ] [MainThread]:   execution_project: sentiment-analysis-410608
[0m14:53:00.731585 [info ] [MainThread]:   schema: youtube_sentiment
[0m14:53:00.731993 [info ] [MainThread]:   location: US
[0m14:53:00.732438 [info ] [MainThread]:   priority: interactive
[0m14:53:00.732840 [info ] [MainThread]:   maximum_bytes_billed: None
[0m14:53:00.733227 [info ] [MainThread]:   impersonate_service_account: None
[0m14:53:00.733642 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m14:53:00.734060 [info ] [MainThread]:   job_retries: 1
[0m14:53:00.734406 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m14:53:00.734762 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m14:53:00.735187 [info ] [MainThread]:   timeout_seconds: 300
[0m14:53:00.735598 [info ] [MainThread]:   client_id: None
[0m14:53:00.736183 [info ] [MainThread]:   token_uri: None
[0m14:53:00.736755 [info ] [MainThread]:   dataproc_region: None
[0m14:53:00.737131 [info ] [MainThread]:   dataproc_cluster_name: None
[0m14:53:00.737496 [info ] [MainThread]:   gcs_bucket: None
[0m14:53:00.737849 [info ] [MainThread]:   dataproc_batch: None
[0m14:53:00.738422 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m14:53:00.739955 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m14:53:00.740355 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:00.740787 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery client: '[Errno 2] No such file or directory: 'sentiment-analysis.privateKey.json''
[0m14:53:00.741226 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m14:53:00.741631 [info ] [MainThread]: [31m1 check failed:[0m
[0m14:53:00.742133 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  [Errno 2] No such file or directory: 'sentiment-analysis.privateKey.json'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m14:53:00.742757 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_wall_clock_time": 0.96636736, "process_user_time": 2.5432, "process_kernel_time": 0.214247, "process_mem_max_rss": "220664", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m14:53:00.743519 [debug] [MainThread]: Command `dbt debug` failed at 14:53:00.743422 after 0.97 seconds
[0m14:53:00.743899 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:53:00.744236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71585c8515d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7158601a3990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71585c7afa90>]}
[0m14:53:00.744607 [debug] [MainThread]: Flushing usage events
[0m14:54:36.984013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x770328fed0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x770328fecc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x770328fef450>]}


============================== 14:54:36.986064 | b63ef6bd-0eda-4dfc-a7d7-6f2531e91013 ==============================
[0m14:54:36.986064 [info ] [MainThread]: Running with dbt=1.8.5
[0m14:54:36.986713 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:54:36.993418 [info ] [MainThread]: dbt version: 1.8.5
[0m14:54:36.993916 [info ] [MainThread]: python version: 3.11.5
[0m14:54:36.994323 [info ] [MainThread]: python path: /home/micasidad/Desktop/yt-comment-analysis/venv/bin/python
[0m14:54:36.994711 [info ] [MainThread]: os info: Linux-6.5.0-44-generic-x86_64-with-glibc2.35
[0m14:54:37.750320 [info ] [MainThread]: Using profiles dir at /home/micasidad/.dbt
[0m14:54:37.750781 [info ] [MainThread]: Using profiles.yml file at /home/micasidad/.dbt/profiles.yml
[0m14:54:37.751184 [info ] [MainThread]: Using dbt_project.yml file at /home/micasidad/Desktop/yt-comment-analysis/dbt/dbt_project.yml
[0m14:54:37.751563 [info ] [MainThread]: adapter type: bigquery
[0m14:54:37.751900 [info ] [MainThread]: adapter version: 1.8.2
[0m14:54:37.815386 [info ] [MainThread]: Configuration:
[0m14:54:37.815833 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:54:37.816202 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:54:37.816526 [info ] [MainThread]: Required dependencies:
[0m14:54:37.816902 [debug] [MainThread]: Executing "git --help"
[0m14:54:37.819025 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:54:37.819860 [debug] [MainThread]: STDERR: "b''"
[0m14:54:37.820150 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:54:37.820490 [info ] [MainThread]: Connection:
[0m14:54:37.820855 [info ] [MainThread]:   method: service-account
[0m14:54:37.821231 [info ] [MainThread]:   database: sentiment-analysis-410608
[0m14:54:37.821791 [info ] [MainThread]:   execution_project: sentiment-analysis-410608
[0m14:54:37.822282 [info ] [MainThread]:   schema: youtube_sentiment
[0m14:54:37.822680 [info ] [MainThread]:   location: US
[0m14:54:37.823055 [info ] [MainThread]:   priority: interactive
[0m14:54:37.823477 [info ] [MainThread]:   maximum_bytes_billed: None
[0m14:54:37.823801 [info ] [MainThread]:   impersonate_service_account: None
[0m14:54:37.824093 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m14:54:37.824385 [info ] [MainThread]:   job_retries: 1
[0m14:54:37.824681 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m14:54:37.824972 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m14:54:37.825265 [info ] [MainThread]:   timeout_seconds: 300
[0m14:54:37.825587 [info ] [MainThread]:   client_id: None
[0m14:54:37.825916 [info ] [MainThread]:   token_uri: None
[0m14:54:37.826241 [info ] [MainThread]:   dataproc_region: None
[0m14:54:37.826583 [info ] [MainThread]:   dataproc_cluster_name: None
[0m14:54:37.826932 [info ] [MainThread]:   gcs_bucket: None
[0m14:54:37.827494 [info ] [MainThread]:   dataproc_batch: None
[0m14:54:37.828287 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m14:54:37.830044 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m14:54:37.830633 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:54:37.876610 [debug] [MainThread]: On debug: select 1 as id
[0m14:54:39.372310 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:500267cd-1891-46bc-ad6f-0ad4fe0bcd01&page=queryresults
[0m14:54:40.081497 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:54:40.082543 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:54:40.083908 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 3.1357725, "process_user_time": 2.600737, "process_kernel_time": 0.195454, "process_mem_max_rss": "221784", "process_in_blocks": "1568", "process_out_blocks": "32"}
[0m14:54:40.085274 [debug] [MainThread]: Command `dbt debug` succeeded at 14:54:40.084988 after 3.14 seconds
[0m14:54:40.086344 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:54:40.087493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77032c9f79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77032ca05e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77032c91a390>]}
[0m14:54:40.088704 [debug] [MainThread]: Flushing usage events
[0m16:01:27.393844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f19752fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f19752d410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f19752d190>]}


============================== 16:01:27.396183 | 115f617d-c570-4f1d-b04a-f302a2c33ba6 ==============================
[0m16:01:27.396183 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:01:27.396684 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:01:28.336974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '115f617d-c570-4f1d-b04a-f302a2c33ba6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f19758c150>]}
[0m16:01:28.382655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '115f617d-c570-4f1d-b04a-f302a2c33ba6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f19925dc50>]}
[0m16:01:28.383299 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:01:28.402580 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:01:28.403803 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:01:28.404842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '115f617d-c570-4f1d-b04a-f302a2c33ba6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f1738d70d0>]}
[0m16:01:29.314157 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.yt_comment_analysis.comments' (models/comments.sql) depends on a node named 'raw_comments' which was not found
[0m16:01:29.314791 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.9628706, "process_user_time": 3.635283, "process_kernel_time": 0.235694, "process_mem_max_rss": "225232", "process_in_blocks": "1856", "process_out_blocks": "16", "command_success": false}
[0m16:01:29.315603 [debug] [MainThread]: Command `dbt run` failed at 16:01:29.315507 after 1.96 seconds
[0m16:01:29.315988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f19af1b990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f1975678d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76f19892cad0>]}
[0m16:01:29.316361 [debug] [MainThread]: Flushing usage events
[0m16:04:32.903584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0de841e090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0de841e150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0de841dcd0>]}


============================== 16:04:32.906032 | e4d412c9-4fe7-4d7d-897a-aa1ea94a9e21 ==============================
[0m16:04:32.906032 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:04:32.906578 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:04:32.915430 [info ] [MainThread]: dbt version: 1.8.5
[0m16:04:32.915958 [info ] [MainThread]: python version: 3.11.5
[0m16:04:32.916352 [info ] [MainThread]: python path: /home/micasidad/Desktop/yt-comment-analysis/venv/bin/python
[0m16:04:32.916742 [info ] [MainThread]: os info: Linux-6.5.0-44-generic-x86_64-with-glibc2.35
[0m16:04:33.816795 [info ] [MainThread]: Using profiles dir at /home/micasidad/.dbt
[0m16:04:33.817325 [info ] [MainThread]: Using profiles.yml file at /home/micasidad/.dbt/profiles.yml
[0m16:04:33.817811 [info ] [MainThread]: Using dbt_project.yml file at /home/micasidad/Desktop/yt-comment-analysis/dbt/dbt_project.yml
[0m16:04:33.818391 [info ] [MainThread]: adapter type: bigquery
[0m16:04:33.818935 [info ] [MainThread]: adapter version: 1.8.2
[0m16:04:33.896849 [info ] [MainThread]: Configuration:
[0m16:04:33.897266 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m16:04:33.897577 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:04:33.897873 [info ] [MainThread]: Required dependencies:
[0m16:04:33.898186 [debug] [MainThread]: Executing "git --help"
[0m16:04:33.900013 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:04:33.900421 [debug] [MainThread]: STDERR: "b''"
[0m16:04:33.900737 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:04:33.901120 [info ] [MainThread]: Connection:
[0m16:04:33.901645 [info ] [MainThread]:   method: service-account
[0m16:04:33.902029 [info ] [MainThread]:   database: sentiment-analysis-410608
[0m16:04:33.902412 [info ] [MainThread]:   execution_project: sentiment-analysis-410608
[0m16:04:33.902804 [info ] [MainThread]:   schema: youtube_sentiment
[0m16:04:33.903448 [info ] [MainThread]:   location: US
[0m16:04:33.904035 [info ] [MainThread]:   priority: interactive
[0m16:04:33.904963 [info ] [MainThread]:   maximum_bytes_billed: None
[0m16:04:33.905793 [info ] [MainThread]:   impersonate_service_account: None
[0m16:04:33.906631 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m16:04:33.907457 [info ] [MainThread]:   job_retries: 1
[0m16:04:33.908267 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m16:04:33.908954 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m16:04:33.909649 [info ] [MainThread]:   timeout_seconds: 300
[0m16:04:33.910324 [info ] [MainThread]:   client_id: None
[0m16:04:33.911020 [info ] [MainThread]:   token_uri: None
[0m16:04:33.911758 [info ] [MainThread]:   dataproc_region: None
[0m16:04:33.912571 [info ] [MainThread]:   dataproc_cluster_name: None
[0m16:04:33.913380 [info ] [MainThread]:   gcs_bucket: None
[0m16:04:33.913970 [info ] [MainThread]:   dataproc_batch: None
[0m16:04:33.914788 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:04:33.917243 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m16:04:33.918055 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:04:33.959957 [debug] [MainThread]: On debug: select 1 as id
[0m16:04:34.981638 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:0ca861f3-0fa2-4957-8af9-b02b8e835404&page=queryresults
[0m16:04:35.641854 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m16:04:35.642591 [info ] [MainThread]: [32mAll checks passed![0m
[0m16:04:35.643235 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 2.7808032, "process_user_time": 2.753865, "process_kernel_time": 0.246733, "process_mem_max_rss": "221976", "process_out_blocks": "24", "process_in_blocks": "0"}
[0m16:04:35.643936 [debug] [MainThread]: Command `dbt debug` succeeded at 16:04:35.643810 after 2.78 seconds
[0m16:04:35.644402 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m16:04:35.644891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0debe179d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0dc4be04d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c0debd36250>]}
[0m16:04:35.645503 [debug] [MainThread]: Flushing usage events
[0m16:06:30.198243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74882432d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74882432cfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74882432ce50>]}


============================== 16:06:30.201580 | 8a64d0d0-16e4-4cd1-81e4-922a4a9aaed6 ==============================
[0m16:06:30.201580 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:06:30.202347 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:06:31.163049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a64d0d0-16e4-4cd1-81e4-922a4a9aaed6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74882438cbd0>]}
[0m16:06:31.214685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a64d0d0-16e4-4cd1-81e4-922a4a9aaed6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748826085f50>]}
[0m16:06:31.215399 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:06:31.222639 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:06:31.223452 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:06:31.223920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8a64d0d0-16e4-4cd1-81e4-922a4a9aaed6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7488006b6810>]}
[0m16:06:32.096360 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.yt_comment_analysis.raw_comments' (models/raw_comments.sql) depends on a source named 'youtube_sentiment.raw_comments' which was not found
[0m16:06:32.097033 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.9420111, "process_user_time": 3.597993, "process_kernel_time": 0.261306, "process_mem_max_rss": "225192", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m16:06:32.097535 [debug] [MainThread]: Command `dbt run` failed at 16:06:32.097444 after 1.94 seconds
[0m16:06:32.097925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748827d63990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748800623690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748800658b10>]}
[0m16:06:32.098310 [debug] [MainThread]: Flushing usage events
[0m16:11:11.716337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a91bb1450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a91bb0f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a91bb0d90>]}


============================== 16:11:11.718567 | a66093b2-e15b-4de8-a7ee-206c56478d34 ==============================
[0m16:11:11.718567 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:11:11.719074 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:11:12.552469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a66093b2-e15b-4de8-a7ee-206c56478d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a6e2efad0>]}
[0m16:11:12.590368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a66093b2-e15b-4de8-a7ee-206c56478d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a938c1f90>]}
[0m16:11:12.590927 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:11:12.597284 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:11:12.598015 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:11:12.598620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a66093b2-e15b-4de8-a7ee-206c56478d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a91ff6150>]}
[0m16:11:13.275823 [error] [MainThread]: Encountered an error:
Compilation Error in model comments (models/comments.sql)
  source() takes exactly two arguments (1 given)
[0m16:11:13.276490 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.5972679, "process_user_time": 3.301758, "process_kernel_time": 0.199381, "process_mem_max_rss": "225000", "process_in_blocks": "16", "process_out_blocks": "16", "command_success": false}
[0m16:11:13.276996 [debug] [MainThread]: Command `dbt run` failed at 16:11:13.276907 after 1.60 seconds
[0m16:11:13.277431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a91bb8210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a955b79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a6e023d10>]}
[0m16:11:13.277824 [debug] [MainThread]: Flushing usage events
[0m16:11:40.833359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0e45b1210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0e45b14d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0e45b0ed0>]}


============================== 16:11:40.835551 | 1a92ad81-eb4a-48bd-9275-3d160b270ed4 ==============================
[0m16:11:40.835551 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:11:40.836131 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:11:41.677984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a92ad81-eb4a-48bd-9275-3d160b270ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0e46c0c90>]}
[0m16:11:41.716083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1a92ad81-eb4a-48bd-9275-3d160b270ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0e62c5f50>]}
[0m16:11:41.716599 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:11:41.723323 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:11:41.724136 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:11:41.724729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1a92ad81-eb4a-48bd-9275-3d160b270ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0c0935990>]}
[0m16:11:42.424591 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.yt_comment_analysis.comments' (models/comments.sql) depends on a source named 'youtube_sentiment.raw_comments' which was not found
[0m16:11:42.425174 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.6290872, "process_user_time": 3.350505, "process_kernel_time": 0.179277, "process_mem_max_rss": "225376", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m16:11:42.425770 [debug] [MainThread]: Command `dbt run` failed at 16:11:42.425686 after 1.63 seconds
[0m16:11:42.426163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0e45e7d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0c073b5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ec0e45a5d90>]}
[0m16:11:42.426515 [debug] [MainThread]: Flushing usage events
[0m16:15:23.939384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc71802d890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc71802d7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc71802d710>]}


============================== 16:15:23.941493 | 7a26cbe8-eb61-407f-9fef-e74458452c3a ==============================
[0m16:15:23.941493 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:15:23.942188 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m16:15:24.774697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a26cbe8-eb61-407f-9fef-e74458452c3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc71808d2d0>]}
[0m16:15:24.812466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a26cbe8-eb61-407f-9fef-e74458452c3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc719d86110>]}
[0m16:15:24.812969 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:15:24.819361 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:15:24.820088 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:15:24.820623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7a26cbe8-eb61-407f-9fef-e74458452c3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc71953e7d0>]}
[0m16:15:25.570566 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.yt_comment_analysis.users' (models/users.sql) depends on a node named 'raw_comments' which was not found
[0m16:15:25.571219 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.6680555, "process_user_time": 3.342773, "process_kernel_time": 0.189061, "process_mem_max_rss": "226504", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m16:15:25.571715 [debug] [MainThread]: Command `dbt run` failed at 16:15:25.571636 after 1.67 seconds
[0m16:15:25.572078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc71ba179d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc718034350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc718034290>]}
[0m16:15:25.572433 [debug] [MainThread]: Flushing usage events
[0m16:16:29.904858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e425d7b5690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e425db38b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e425d7b5a90>]}


============================== 16:16:29.906912 | 5e9faf3d-bbe0-4edf-b5ea-7cc5ccef32dd ==============================
[0m16:16:29.906912 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:16:29.907509 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:16:30.745511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5e9faf3d-bbe0-4edf-b5ea-7cc5ccef32dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4239c1a2d0>]}
[0m16:16:30.783013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5e9faf3d-bbe0-4edf-b5ea-7cc5ccef32dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4239b4aad0>]}
[0m16:16:30.783596 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:16:30.790012 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:16:30.790767 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:16:30.791395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5e9faf3d-bbe0-4edf-b5ea-7cc5ccef32dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4239b45990>]}
[0m16:16:31.469296 [error] [MainThread]: Encountered an error:
Compilation Error in model users (models/users.sql)
  expected token ':', got '}'
    line 8
      {{{ source('youtube_sentiment', 'raw_comments') }}  -- Referencing the raw_comments table
[0m16:16:31.469874 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.6014694, "process_user_time": 3.255558, "process_kernel_time": 0.210934, "process_mem_max_rss": "225248", "process_out_blocks": "8", "command_success": false, "process_in_blocks": "0"}
[0m16:16:31.470421 [debug] [MainThread]: Command `dbt run` failed at 16:16:31.470328 after 1.60 seconds
[0m16:16:31.470793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e42611b79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e425d7bc1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4239a83d50>]}
[0m16:16:31.471144 [debug] [MainThread]: Flushing usage events
[0m16:16:52.568206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782d0c5a4890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782d0c249690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782d0c24a250>]}


============================== 16:16:52.570632 | b9445687-d499-4f12-947c-5bbbecc83100 ==============================
[0m16:16:52.570632 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:16:52.571438 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m16:16:53.421966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782cec6d2390>]}
[0m16:16:53.460301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782d0df55f50>]}
[0m16:16:53.460846 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:16:53.467951 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:16:53.468716 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:16:53.469190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782cec5fb610>]}
[0m16:16:54.340698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782cec55c450>]}
[0m16:16:54.507368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782cec30d5d0>]}
[0m16:16:54.507859 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:16:54.508208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782ce735f210>]}
[0m16:16:54.509733 [info ] [MainThread]: 
[0m16:16:54.510240 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:16:54.513440 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:16:54.513979 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:16:55.471903 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:16:55.472363 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:16:56.100874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782ce735f210>]}
[0m16:16:56.101979 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:16:56.103064 [info ] [MainThread]: 
[0m16:16:56.114496 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:16:56.115655 [info ] [Thread-1 (]: 1 of 3 START sql table model youtube_sentiment.comments ........................ [RUN]
[0m16:16:56.116801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:16:56.117597 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:16:56.130676 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:16:56.132052 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:16:56.150847 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:56.911260 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.comments"
[0m16:16:56.912138 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.comments"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`comments`
      
    
    

    OPTIONS()
    as (
      -- models/comments.sql

WITH comment_data AS (
    SELECT
        VIDEO_ID,
        REPLY_COUNT,
        AUTHOR,
        clean_text(TEXT) AS TEXT,
        LIKE_COUNT,
        CAST(PUBLISHED_AT AS TIMESTAMP) AS PUBLISHED_AT,
        get_sentiment_scoree(clean_text(TEXT)) AS SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
)

SELECT * FROM comment_data;
    );
  
[0m16:16:57.292724 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:2315e672-b8b3-442f-81d1-889a0e087b2d&page=queryresults
[0m16:16:57.293267 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [28:27]; reason: invalidQuery, location: query, message: Syntax error: Expected ")" but got ";" at [28:27]')
[0m16:16:58.114970 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:2e57d31c-7a76-4e00-a2d1-96e0a9df7dc6&page=queryresults
[0m16:16:58.117701 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:2e57d31c-7a76-4e00-a2d1-96e0a9df7dc6&page=queryresults
[0m16:16:58.149157 [debug] [Thread-1 (]: Database Error in model comments (models/comments.sql)
  Syntax error: Expected ")" but got ";" at [28:27]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:16:58.154803 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782cec50ee10>]}
[0m16:16:58.157554 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model youtube_sentiment.comments ............... [[31mERROR[0m in 2.03s]
[0m16:16:58.159638 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:16:58.161060 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:16:58.165001 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:16:58.166673 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:16:58.167592 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:16:58.172857 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:16:58.173780 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:16:58.177049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:58.790958 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:16:58.792011 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data;
    );
  
[0m16:16:59.139564 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:c25fb263-f81c-4912-baa8-cd33169ed4ff&page=queryresults
[0m16:16:59.142049 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [25:24]; reason: invalidQuery, location: query, message: Syntax error: Expected ")" but got ";" at [25:24]')
[0m16:16:59.547834 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:856bdd12-159b-4216-968b-e78fa1a5554e&page=queryresults
[0m16:16:59.549130 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:856bdd12-159b-4216-968b-e78fa1a5554e&page=queryresults
[0m16:16:59.553606 [debug] [Thread-1 (]: Database Error in model users (models/users.sql)
  Syntax error: Expected ")" but got ";" at [25:24]
  compiled Code at target/run/yt_comment_analysis/models/users.sql
[0m16:16:59.554272 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9445687-d499-4f12-947c-5bbbecc83100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782cec290250>]}
[0m16:16:59.555258 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model youtube_sentiment.users .................. [[31mERROR[0m in 1.39s]
[0m16:16:59.556355 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:16:59.557015 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:16:59.557724 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:16:59.558416 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:16:59.559933 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:16:59.560410 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:16:59.560945 [info ] [MainThread]: 
[0m16:16:59.561479 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 5.05 seconds (5.05s).
[0m16:16:59.562590 [debug] [MainThread]: Command end result
[0m16:16:59.594137 [info ] [MainThread]: 
[0m16:16:59.594604 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:16:59.594950 [info ] [MainThread]: 
[0m16:16:59.595370 [error] [MainThread]:   Database Error in model comments (models/comments.sql)
  Syntax error: Expected ")" but got ";" at [28:27]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:16:59.595693 [info ] [MainThread]: 
[0m16:16:59.596092 [error] [MainThread]:   Database Error in model users (models/users.sql)
  Syntax error: Expected ")" but got ";" at [25:24]
  compiled Code at target/run/yt_comment_analysis/models/users.sql
[0m16:16:59.596422 [info ] [MainThread]: 
[0m16:16:59.596806 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=1 TOTAL=3
[0m16:16:59.597268 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 7.068395, "process_user_time": 4.034783, "process_kernel_time": 0.26497, "process_mem_max_rss": "231116", "process_in_blocks": "1056", "process_out_blocks": "2896", "command_success": false}
[0m16:16:59.597793 [debug] [MainThread]: Command `dbt run` failed at 16:16:59.597703 after 7.07 seconds
[0m16:16:59.598198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782d0fc179d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782d0fcba810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x782cec983c90>]}
[0m16:16:59.598597 [debug] [MainThread]: Flushing usage events
[0m16:18:31.062202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c005df1710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c005e1d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c005e1f910>]}


============================== 16:18:31.064450 | fc96e2dd-b069-4c7b-80ba-b2d58f040735 ==============================
[0m16:18:31.064450 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:18:31.065057 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:18:31.906775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74bfe20bf750>]}
[0m16:18:31.946323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c007b05c50>]}
[0m16:18:31.946886 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:18:31.953403 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:18:32.032818 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:18:32.033203 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:18:32.057373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c00622b1d0>]}
[0m16:18:32.124448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74bfe1e92750>]}
[0m16:18:32.124900 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:18:32.125275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74bfe1f33850>]}
[0m16:18:32.126611 [info ] [MainThread]: 
[0m16:18:32.127208 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:18:32.130183 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:18:32.130548 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:18:33.146187 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:18:33.148172 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:18:33.743499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74bfe281b7d0>]}
[0m16:18:33.744070 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:18:33.744442 [info ] [MainThread]: 
[0m16:18:33.746509 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:18:33.747013 [info ] [Thread-1 (]: 1 of 3 START sql table model youtube_sentiment.comments ........................ [RUN]
[0m16:18:33.747509 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:18:33.747901 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:18:33.755421 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:18:33.756174 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:18:33.788018 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.comments"
[0m16:18:33.788721 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:33.820330 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.comments"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`comments`
      
    
    

    OPTIONS()
    as (
      -- models/comments.sql

WITH comment_data AS (
    SELECT
        VIDEO_ID,
        REPLY_COUNT,
        AUTHOR,
        clean_text(TEXT) AS TEXT,
        LIKE_COUNT,
        CAST(PUBLISHED_AT AS TIMESTAMP) AS PUBLISHED_AT,
        get_sentiment_scoree(clean_text(TEXT)) AS SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
)

SELECT * FROM comment_data;
    );
  
[0m16:18:34.682820 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:39a8eb10-2af7-4414-bdef-f10ac7adf4dc&page=queryresults
[0m16:18:34.685341 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [28:27]; reason: invalidQuery, location: query, message: Syntax error: Expected ")" but got ";" at [28:27]')
[0m16:18:35.086366 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:c10075f1-d6c6-419f-9d19-78d8fabe7721&page=queryresults
[0m16:18:35.087081 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:c10075f1-d6c6-419f-9d19-78d8fabe7721&page=queryresults
[0m16:18:35.092427 [debug] [Thread-1 (]: Database Error in model comments (models/comments.sql)
  Syntax error: Expected ")" but got ";" at [28:27]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:18:35.094079 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74bfe1dca710>]}
[0m16:18:35.094962 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model youtube_sentiment.comments ............... [[31mERROR[0m in 1.35s]
[0m16:18:35.096015 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:18:35.097010 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:18:35.098785 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:18:35.099749 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:18:35.100731 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:18:35.105824 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:18:35.107015 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:18:35.110712 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:18:35.111468 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:35.146579 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data;
    );
  
[0m16:18:36.024311 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:3917b677-e805-4bcf-836f-88348270f2f1&page=queryresults
[0m16:18:36.026505 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [25:24]; reason: invalidQuery, location: query, message: Syntax error: Expected ")" but got ";" at [25:24]')
[0m16:18:36.418657 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:095870ad-f54d-4e53-9ca7-8ac67370cb6d&page=queryresults
[0m16:18:36.420797 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:095870ad-f54d-4e53-9ca7-8ac67370cb6d&page=queryresults
[0m16:18:36.424593 [debug] [Thread-1 (]: Database Error in model users (models/users.sql)
  Syntax error: Expected ")" but got ";" at [25:24]
  compiled Code at target/run/yt_comment_analysis/models/users.sql
[0m16:18:36.425181 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc96e2dd-b069-4c7b-80ba-b2d58f040735', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74bfe0443790>]}
[0m16:18:36.425964 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model youtube_sentiment.users .................. [[31mERROR[0m in 1.33s]
[0m16:18:36.426848 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:18:36.427450 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:18:36.428181 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:18:36.428834 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:18:36.430207 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:18:36.430652 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:18:36.431152 [info ] [MainThread]: 
[0m16:18:36.431646 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 4.30 seconds (4.30s).
[0m16:18:36.432639 [debug] [MainThread]: Command end result
[0m16:18:36.463329 [info ] [MainThread]: 
[0m16:18:36.463795 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:18:36.464134 [info ] [MainThread]: 
[0m16:18:36.464540 [error] [MainThread]:   Database Error in model comments (models/comments.sql)
  Syntax error: Expected ")" but got ";" at [28:27]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:18:36.464871 [info ] [MainThread]: 
[0m16:18:36.465255 [error] [MainThread]:   Database Error in model users (models/users.sql)
  Syntax error: Expected ")" but got ";" at [25:24]
  compiled Code at target/run/yt_comment_analysis/models/users.sql
[0m16:18:36.465602 [info ] [MainThread]: 
[0m16:18:36.465980 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=1 TOTAL=3
[0m16:18:36.466459 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.442516, "process_user_time": 3.199282, "process_kernel_time": 0.190764, "process_mem_max_rss": "228324", "process_out_blocks": "1952", "command_success": false, "process_in_blocks": "0"}
[0m16:18:36.466937 [debug] [MainThread]: Command `dbt run` failed at 16:18:36.466855 after 5.44 seconds
[0m16:18:36.467312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c005e1d790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c0097d79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74c00987a6d0>]}
[0m16:18:36.467664 [debug] [MainThread]: Flushing usage events
[0m16:21:47.309219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0bd0657d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0bd092310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0bd093910>]}


============================== 16:21:47.311287 | 0b934745-dff1-4119-9a8f-b6c40ce68134 ==============================
[0m16:21:47.311287 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:21:47.312027 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:21:48.167864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0998037d0>]}
[0m16:21:48.210924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0bed966d0>]}
[0m16:21:48.211676 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:21:48.220567 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:21:48.303676 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:21:48.304196 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.sql
[0m16:21:48.473387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce099045710>]}
[0m16:21:48.531879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce09812ba90>]}
[0m16:21:48.532371 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:21:48.532784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0997bc210>]}
[0m16:21:48.534247 [info ] [MainThread]: 
[0m16:21:48.534884 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:21:48.537882 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:21:48.538540 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:21:49.528182 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:21:49.530375 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:21:50.242266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce099066d50>]}
[0m16:21:50.245024 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:21:50.245936 [info ] [MainThread]: 
[0m16:21:50.249095 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:21:50.249810 [info ] [Thread-1 (]: 1 of 3 START sql table model youtube_sentiment.comments ........................ [RUN]
[0m16:21:50.250508 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:21:50.251151 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:21:50.260792 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:21:50.261600 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:21:50.315666 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.comments"
[0m16:21:50.317052 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:21:50.394059 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.comments"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`comments`
      
    
    

    OPTIONS()
    as (
      -- models/comments.sql

WITH comment_data AS (
    SELECT
        VIDEO_ID,
        REPLY_COUNT,
        AUTHOR,
        clean_text(TEXT) AS TEXT,
        LIKE_COUNT,
        CAST(PUBLISHED_AT AS TIMESTAMP) AS PUBLISHED_AT,
        get_sentiment_score(clean_text(TEXT)) AS SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
)

SELECT * FROM comment_data;
    );
  
[0m16:21:51.290050 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:af04cf31-fb98-480a-b331-47c00d28a9d3&page=queryresults
[0m16:21:51.292070 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [28:27]; reason: invalidQuery, location: query, message: Syntax error: Expected ")" but got ";" at [28:27]')
[0m16:21:52.005898 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:0a9f5d61-b075-4107-8c1e-163d4891f08c&page=queryresults
[0m16:21:52.008303 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:0a9f5d61-b075-4107-8c1e-163d4891f08c&page=queryresults
[0m16:21:52.014429 [debug] [Thread-1 (]: Database Error in model comments (models/comments.sql)
  Syntax error: Expected ")" but got ";" at [28:27]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:21:52.016224 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0990af750>]}
[0m16:21:52.017020 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model youtube_sentiment.comments ............... [[31mERROR[0m in 1.76s]
[0m16:21:52.017857 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:21:52.018411 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:21:52.019813 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:21:52.020494 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:21:52.021016 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:21:52.024966 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:21:52.025971 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:21:52.029975 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:21:52.030797 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:21:52.068634 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data;
    );
  
[0m16:21:52.928105 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:f13742b5-a4de-46ab-8970-85b14afd0a31&page=queryresults
[0m16:21:52.930521 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [25:24]; reason: invalidQuery, location: query, message: Syntax error: Expected ")" but got ";" at [25:24]')
[0m16:21:53.849406 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:802fff51-e1e3-4841-921e-51c1b8b85d5c&page=queryresults
[0m16:21:53.851895 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:802fff51-e1e3-4841-921e-51c1b8b85d5c&page=queryresults
[0m16:21:53.856935 [debug] [Thread-1 (]: Database Error in model users (models/users.sql)
  Syntax error: Expected ")" but got ";" at [25:24]
  compiled Code at target/run/yt_comment_analysis/models/users.sql
[0m16:21:53.857637 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b934745-dff1-4119-9a8f-b6c40ce68134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0981ee3d0>]}
[0m16:21:53.858690 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model youtube_sentiment.users .................. [[31mERROR[0m in 1.84s]
[0m16:21:53.859729 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:21:53.860457 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:21:53.861295 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:21:53.861994 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:21:53.863662 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:21:53.864231 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:21:53.864905 [info ] [MainThread]: 
[0m16:21:53.865562 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 5.33 seconds (5.33s).
[0m16:21:53.866918 [debug] [MainThread]: Command end result
[0m16:21:53.902127 [info ] [MainThread]: 
[0m16:21:53.902837 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:21:53.903343 [info ] [MainThread]: 
[0m16:21:53.903773 [error] [MainThread]:   Database Error in model comments (models/comments.sql)
  Syntax error: Expected ")" but got ";" at [28:27]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:21:53.904100 [info ] [MainThread]: 
[0m16:21:53.904535 [error] [MainThread]:   Database Error in model users (models/users.sql)
  Syntax error: Expected ")" but got ";" at [25:24]
  compiled Code at target/run/yt_comment_analysis/models/users.sql
[0m16:21:53.904951 [info ] [MainThread]: 
[0m16:21:53.905459 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=1 TOTAL=3
[0m16:21:53.906401 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 6.6340356, "process_user_time": 3.371363, "process_kernel_time": 0.228499, "process_mem_max_rss": "231160", "process_out_blocks": "2904", "command_success": false, "process_in_blocks": "0"}
[0m16:21:53.907049 [debug] [MainThread]: Command `dbt run` failed at 16:21:53.906926 after 6.63 seconds
[0m16:21:53.907572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0bd06c210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0c0a179d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ce0bd09bed0>]}
[0m16:21:53.907977 [debug] [MainThread]: Flushing usage events
[0m16:23:11.761787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f6da556d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f6da55490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f6da55010>]}


============================== 16:23:11.764460 | 215dfb2f-d239-47ea-b2ed-948d62977962 ==============================
[0m16:23:11.764460 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:23:11.764975 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:23:12.660446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f49d74190>]}
[0m16:23:12.698577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f6f766610>]}
[0m16:23:12.699117 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:23:12.705933 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:23:12.785163 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m16:23:12.785708 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/videos.sql
[0m16:23:12.786108 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.sql
[0m16:23:12.786492 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/users.sql
[0m16:23:12.956768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f49cea050>]}
[0m16:23:13.015446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f48bb78d0>]}
[0m16:23:13.015928 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:23:13.016354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f6f4b5d50>]}
[0m16:23:13.017779 [info ] [MainThread]: 
[0m16:23:13.018345 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:23:13.021380 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:23:13.021895 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:23:14.135592 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:23:14.137662 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:23:14.812623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f4a4cf190>]}
[0m16:23:14.813699 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:23:14.814352 [info ] [MainThread]: 
[0m16:23:14.817449 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:23:14.818442 [info ] [Thread-1 (]: 1 of 3 START sql table model youtube_sentiment.comments ........................ [RUN]
[0m16:23:14.819341 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:23:14.820105 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:23:14.828070 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:23:14.828666 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:23:14.858919 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.comments"
[0m16:23:14.859522 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:23:14.891247 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.comments"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`comments`
      
    
    

    OPTIONS()
    as (
      -- models/comments.sql

WITH comment_data AS (
    SELECT
        VIDEO_ID,
        REPLY_COUNT,
        AUTHOR,
        clean_text(TEXT) AS TEXT,
        LIKE_COUNT,
        CAST(PUBLISHED_AT AS TIMESTAMP) AS PUBLISHED_AT,
        get_sentiment_score(clean_text(TEXT)) AS SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
)

SELECT * FROM comment_data
    );
  
[0m16:23:15.674501 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:42da284b-0233-45c2-9806-9312776ed796&page=queryresults
[0m16:23:15.676189 [debug] [Thread-1 (]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.comments"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`comments`
      
    
    

    OPTIONS()
    as (
      -- models/comments.sql

WITH comment_data AS (
    SELECT
        VIDEO_ID,
        REPLY_COUNT,
        AUTHOR,
        clean_text(TEXT) AS TEXT,
        LIKE_COUNT,
        CAST(PUBLISHED_AT AS TIMESTAMP) AS PUBLISHED_AT,
        get_sentiment_score(clean_text(TEXT)) AS SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
)

SELECT * FROM comment_data
    );
  
[0m16:23:15.676933 [debug] [Thread-1 (]: BigQuery adapter: 404 Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US; reason: notFound, message: Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US

Location: US
Job ID: 42da284b-0233-45c2-9806-9312776ed796

[0m16:23:15.683537 [debug] [Thread-1 (]: Runtime Error in model comments (models/comments.sql)
  404 Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US; reason: notFound, message: Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US
  
  Location: US
  Job ID: 42da284b-0233-45c2-9806-9312776ed796
  
[0m16:23:15.685170 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f48c16d10>]}
[0m16:23:15.685969 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model youtube_sentiment.comments ............... [[31mERROR[0m in 0.86s]
[0m16:23:15.686773 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:23:15.687374 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:23:15.688809 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:23:15.689430 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:23:15.689922 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:23:15.693574 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:23:15.694344 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:23:15.697719 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:23:15.698538 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:23:15.738982 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:23:16.590291 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:US:f178e0cd-392d-4748-aa17-7499baeeff57&page=queryresults
[0m16:23:16.591984 [debug] [Thread-1 (]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:23:16.592609 [debug] [Thread-1 (]: BigQuery adapter: 404 Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US; reason: notFound, message: Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US

Location: US
Job ID: f178e0cd-392d-4748-aa17-7499baeeff57

[0m16:23:16.596259 [debug] [Thread-1 (]: Runtime Error in model users (models/users.sql)
  404 Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US; reason: notFound, message: Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US
  
  Location: US
  Job ID: f178e0cd-392d-4748-aa17-7499baeeff57
  
[0m16:23:16.596848 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '215dfb2f-d239-47ea-b2ed-948d62977962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f4814fc50>]}
[0m16:23:16.597609 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model youtube_sentiment.users .................. [[31mERROR[0m in 0.91s]
[0m16:23:16.598353 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:23:16.598928 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:23:16.599581 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:23:16.600140 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:23:16.601410 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:23:16.601834 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:23:16.602299 [info ] [MainThread]: 
[0m16:23:16.602750 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 3.58 seconds (3.58s).
[0m16:23:16.603759 [debug] [MainThread]: Command end result
[0m16:23:16.634230 [info ] [MainThread]: 
[0m16:23:16.634825 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:23:16.635486 [info ] [MainThread]: 
[0m16:23:16.635992 [error] [MainThread]:   Runtime Error in model comments (models/comments.sql)
  404 Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US; reason: notFound, message: Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US
  
  Location: US
  Job ID: 42da284b-0233-45c2-9806-9312776ed796
  
[0m16:23:16.636502 [info ] [MainThread]: 
[0m16:23:16.637163 [error] [MainThread]:   Runtime Error in model users (models/users.sql)
  404 Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US; reason: notFound, message: Not found: Dataset sentiment-analysis-410608:youtube_sentiment was not found in location US
  
  Location: US
  Job ID: f178e0cd-392d-4748-aa17-7499baeeff57
  
[0m16:23:16.637737 [info ] [MainThread]: 
[0m16:23:16.638429 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=1 TOTAL=3
[0m16:23:16.639223 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 4.919747, "process_user_time": 3.162602, "process_kernel_time": 0.245814, "process_mem_max_rss": "230744", "process_out_blocks": "2920", "command_success": false, "process_in_blocks": "0"}
[0m16:23:16.639716 [debug] [MainThread]: Command `dbt run` failed at 16:23:16.639628 after 4.92 seconds
[0m16:23:16.640119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f714179d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f6da55490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775f714ba810>]}
[0m16:23:16.640745 [debug] [MainThread]: Flushing usage events
[0m16:27:13.885658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c503d5910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c50403910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c503d59d0>]}


============================== 16:27:13.887759 | 9ef0ce86-c680-4382-b67d-a10ab811119d ==============================
[0m16:27:13.887759 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:27:13.888315 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:27:14.722271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c2c6e2450>]}
[0m16:27:14.759945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c520edfd0>]}
[0m16:27:14.760503 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:27:14.767087 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:27:14.831510 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:27:14.832032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c518caf10>]}
[0m16:27:15.709506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c2c9a77d0>]}
[0m16:27:15.777727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c2c76d690>]}
[0m16:27:15.778199 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:27:15.778599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c53a70ad0>]}
[0m16:27:15.780009 [info ] [MainThread]: 
[0m16:27:15.780513 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:27:15.783571 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:27:15.784091 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:27:16.826096 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:27:16.828078 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:27:17.484511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c2b4fd910>]}
[0m16:27:17.486884 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:27:17.487772 [info ] [MainThread]: 
[0m16:27:17.490250 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:27:17.491018 [info ] [Thread-1 (]: 1 of 3 START sql table model youtube_sentiment.comments ........................ [RUN]
[0m16:27:17.491743 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:27:17.492333 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:27:17.501400 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:27:17.502159 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:27:17.535655 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.comments"
[0m16:27:17.536231 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:27:17.567879 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.comments"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`comments`
      
    
    

    OPTIONS()
    as (
      -- models/comments.sql

WITH comment_data AS (
    SELECT
        VIDEO_ID,
        REPLY_COUNT,
        AUTHOR,
        clean_text(TEXT) AS TEXT,
        LIKE_COUNT,
        CAST(PUBLISHED_AT AS TIMESTAMP) AS PUBLISHED_AT,
        get_sentiment_score(clean_text(TEXT)) AS SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
)

SELECT * FROM comment_data
    );
  
[0m16:27:19.179406 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:5bac761f-24e7-4926-a067-c74d4000b059&page=queryresults
[0m16:27:19.589002 [debug] [Thread-1 (]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Function not found: clean_text at [20:9]; reason: invalidQuery, location: query, message: Function not found: clean_text at [20:9]')
[0m16:27:21.240174 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:e3ad6e2a-433f-4d51-854e-5ab98ccc390b&page=queryresults
[0m16:27:21.598985 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:e3ad6e2a-433f-4d51-854e-5ab98ccc390b&page=queryresults
[0m16:27:21.614193 [debug] [Thread-1 (]: Database Error in model comments (models/comments.sql)
  Function not found: clean_text at [20:9]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:27:21.615924 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c2c7a6550>]}
[0m16:27:21.616720 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model youtube_sentiment.comments ............... [[31mERROR[0m in 4.12s]
[0m16:27:21.617496 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:27:21.618186 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:27:21.620149 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:27:21.620863 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:27:21.621422 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:27:21.627704 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:27:21.628501 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:27:21.631623 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:27:21.632239 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:27:21.668732 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:27:23.888222 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:42487c01-a28b-434c-a00f-c61a5b5af253&page=queryresults
[0m16:27:26.033896 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9ef0ce86-c680-4382-b67d-a10ab811119d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c2c4ec850>]}
[0m16:27:26.034633 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.41s]
[0m16:27:26.042358 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:27:26.042937 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:27:26.043439 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:27:26.044093 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:27:26.045359 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:27:26.045775 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:27:26.046263 [info ] [MainThread]: 
[0m16:27:26.046725 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 10.27 seconds (10.27s).
[0m16:27:26.047767 [debug] [MainThread]: Command end result
[0m16:27:26.074346 [info ] [MainThread]: 
[0m16:27:26.074778 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:27:26.075091 [info ] [MainThread]: 
[0m16:27:26.075486 [error] [MainThread]:   Database Error in model comments (models/comments.sql)
  Function not found: clean_text at [20:9]
  compiled Code at target/run/yt_comment_analysis/models/comments.sql
[0m16:27:26.075975 [info ] [MainThread]: 
[0m16:27:26.076582 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m16:27:26.077027 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 12.228312, "process_user_time": 4.040099, "process_kernel_time": 0.216864, "process_mem_max_rss": "231116", "process_out_blocks": "2904", "command_success": false, "process_in_blocks": "0"}
[0m16:27:26.077486 [debug] [MainThread]: Command `dbt run` failed at 16:27:26.077403 after 12.23 seconds
[0m16:27:26.077853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c5040bd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c53dd79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c53e7ab90>]}
[0m16:27:26.078206 [debug] [MainThread]: Flushing usage events
[0m16:52:59.331771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db254629c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db254656bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db254656350>]}


============================== 16:52:59.333803 | 3468032f-206a-4fab-9d28-071ed62f477e ==============================
[0m16:52:59.333803 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:52:59.334521 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m16:53:00.167254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3468032f-206a-4fab-9d28-071ed62f477e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db230954650>]}
[0m16:53:00.205144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3468032f-206a-4fab-9d28-071ed62f477e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db256356090>]}
[0m16:53:00.205698 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:53:00.212281 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:53:00.290498 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m16:53:00.290994 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/videos.py
[0m16:53:00.291376 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/comments.py
[0m16:53:00.291719 [debug] [MainThread]: Partial parsing: deleted file: yt_comment_analysis://models/comments.sql
[0m16:53:00.292035 [debug] [MainThread]: Partial parsing: deleted file: yt_comment_analysis://models/videos.sql
[0m16:53:00.408611 [error] [MainThread]: Encountered an error:
Parsing Error in model videos (models/videos.py)
  model function should have two args, `dbt` and a session to current warehouse
[0m16:53:00.409256 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.1141106, "process_user_time": 2.798166, "process_kernel_time": 0.183094, "process_mem_max_rss": "225508", "process_out_blocks": "8", "command_success": false, "process_in_blocks": "0"}
[0m16:53:00.409722 [debug] [MainThread]: Command `dbt run` failed at 16:53:00.409643 after 1.11 seconds
[0m16:53:00.410103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db25465df50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db2580179d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db2580bab90>]}
[0m16:53:00.410456 [debug] [MainThread]: Flushing usage events
[0m16:55:09.242190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a20363d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a2038fad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a2038fdd0>]}


============================== 16:55:09.244277 | 91cf5845-f731-4dd9-af0f-72ff134997e4 ==============================
[0m16:55:09.244277 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:55:09.244965 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:55:10.080916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '91cf5845-f731-4dd9-af0f-72ff134997e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a203c4910>]}
[0m16:55:10.118489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '91cf5845-f731-4dd9-af0f-72ff134997e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a22075e90>]}
[0m16:55:10.119022 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:55:10.125435 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:55:10.203641 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m16:55:10.204113 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/comments.py
[0m16:55:10.204440 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/videos.py
[0m16:55:10.204729 [debug] [MainThread]: Partial parsing: deleted file: yt_comment_analysis://models/videos.sql
[0m16:55:10.205021 [debug] [MainThread]: Partial parsing: deleted file: yt_comment_analysis://models/comments.sql
[0m16:55:10.334368 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.yt_comment_analysis.comments' (models/comments.py) depends on a node named 'raw_comments' which was not found
[0m16:55:10.334979 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.1294538, "process_user_time": 2.768645, "process_kernel_time": 0.205532, "process_mem_max_rss": "225492", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m16:55:10.335491 [debug] [MainThread]: Command `dbt run` failed at 16:55:10.335412 after 1.13 seconds
[0m16:55:10.335879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a23d1b990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a20363e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740a2218a4d0>]}
[0m16:55:10.336257 [debug] [MainThread]: Flushing usage events
[0m16:56:15.383914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30a4e558d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30a6053550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30a4e54950>]}


============================== 16:56:15.385954 | 5fbbe2bf-02fd-45bb-ab3d-f42b24b11826 ==============================
[0m16:56:15.385954 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:56:15.386603 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:56:16.216638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3081188410>]}
[0m16:56:16.254017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30a6b85fd0>]}
[0m16:56:16.254564 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:56:16.261093 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:56:16.338853 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m16:56:16.339346 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/videos.py
[0m16:56:16.339688 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/comments.py
[0m16:56:16.339981 [debug] [MainThread]: Partial parsing: deleted file: yt_comment_analysis://models/videos.sql
[0m16:56:16.340322 [debug] [MainThread]: Partial parsing: deleted file: yt_comment_analysis://models/comments.sql
[0m16:56:16.508142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3080ea7650>]}
[0m16:56:16.565891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3080d27a10>]}
[0m16:56:16.566357 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:56:16.566711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3080ee3c50>]}
[0m16:56:16.568071 [info ] [MainThread]: 
[0m16:56:16.568563 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:56:16.571374 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:56:16.571842 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:56:17.875396 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:56:17.877286 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:56:18.552050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3081188b90>]}
[0m16:56:18.554227 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:56:18.555818 [info ] [MainThread]: 
[0m16:56:18.566391 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:56:18.568634 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m16:56:18.570651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:56:18.572513 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:56:18.617663 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:56:18.618297 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:56:18.645276 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m16:56:18.645915 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","None")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m16:56:18.646575 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Need to supply dataproc_region in profile to submit python job
[0m16:56:18.657370 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1533, in submit_python_job
    job_helper = self.python_submission_helpers[submission_method](
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 43, in __init__
    raise ValueError(
ValueError: Need to supply dataproc_region in profile to submit python job

[0m16:56:18.658803 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a3080dcbdd0>]}
[0m16:56:18.659850 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 0.09s]
[0m16:56:18.660624 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:56:18.661056 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:56:18.662355 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:56:18.663002 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:56:18.663507 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:56:18.666270 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:56:18.666845 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:56:18.668826 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:56:19.441314 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:56:19.450789 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:56:19.921520 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:dd19fd1c-39dc-4298-afca-9125e5a6c8b3&page=queryresults
[0m16:56:22.449060 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbbe2bf-02fd-45bb-ab3d-f42b24b11826', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30a6899a90>]}
[0m16:56:22.449993 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.79s]
[0m16:56:22.450957 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:56:22.451647 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:56:22.452335 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:56:22.453038 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:56:22.454419 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:56:22.454848 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:56:22.455353 [info ] [MainThread]: 
[0m16:56:22.455820 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 5.89 seconds (5.89s).
[0m16:56:22.456877 [debug] [MainThread]: Command end result
[0m16:56:22.482200 [info ] [MainThread]: 
[0m16:56:22.482646 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:56:22.482959 [info ] [MainThread]: 
[0m16:56:22.483340 [error] [MainThread]:   Need to supply dataproc_region in profile to submit python job
[0m16:56:22.483648 [info ] [MainThread]: 
[0m16:56:22.483974 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m16:56:22.484508 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 7.1369953, "process_user_time": 3.309658, "process_kernel_time": 0.229931, "process_mem_max_rss": "231104", "process_in_blocks": "152", "process_out_blocks": "2944", "command_success": false}
[0m16:56:22.485222 [debug] [MainThread]: Command `dbt run` failed at 16:56:22.485133 after 7.14 seconds
[0m16:56:22.485633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30a88179d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30a4e48fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a30812d2a50>]}
[0m16:56:22.486078 [debug] [MainThread]: Flushing usage events
[0m17:02:49.113143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a7721c95d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a7721cae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a772889f90>]}


============================== 17:02:49.115265 | 9e046d69-f907-4558-952a-57cc6af94f41 ==============================
[0m17:02:49.115265 [info ] [MainThread]: Running with dbt=1.5.1
[0m17:02:49.115766 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:02:49.851286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e046d69-f907-4558-952a-57cc6af94f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a7726aa190>]}
[0m17:02:49.864069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e046d69-f907-4558-952a-57cc6af94f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a74ed48490>]}
[0m17:02:49.871743 [debug] [MainThread]: checksum: 51f6b581eba8f8101bc020bf9faf8f96af641e2da86d581f66e2bdf0ff384b1c, vars: {}, profile: , target: , version: 1.5.1
[0m17:02:49.879662 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:02:49.880154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9e046d69-f907-4558-952a-57cc6af94f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a74e98c090>]}
[0m17:02:50.010819 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "drop_relation" in the project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/adapters/relation.sql
      - macros/relations/drop.sql
[0m17:02:50.011402 [debug] [MainThread]: Command `dbt run` failed at 17:02:50.011321 after 0.90 seconds
[0m17:02:50.011671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a772859dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a7721b6f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76a74ea48fd0>]}
[0m17:02:50.011965 [debug] [MainThread]: Flushing usage events
[0m10:29:04.480280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d62eb7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d62eebe90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d62eea510>]}


============================== 10:29:04.482630 | 5944c15d-a723-41cd-bed5-c3beaa32f324 ==============================
[0m10:29:04.482630 [info ] [MainThread]: Running with dbt=1.8.5
[0m10:29:04.483386 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:29:05.175734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d62efed10>]}
[0m10:29:05.216812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d64bf8e50>]}
[0m10:29:05.217394 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m10:29:05.224549 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m10:29:05.313828 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:29:05.314210 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:29:05.416956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d64387750>]}
[0m10:29:05.488427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d44516550>]}
[0m10:29:05.488910 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m10:29:05.489323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d63b445d0>]}
[0m10:29:05.490887 [info ] [MainThread]: 
[0m10:29:05.491510 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:29:05.494488 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m10:29:05.494866 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:29:06.580251 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m10:29:06.582483 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:29:07.401076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d44a04310>]}
[0m10:29:07.403529 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:29:07.404126 [info ] [MainThread]: 
[0m10:29:07.415396 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m10:29:07.416537 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m10:29:07.417216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m10:29:07.417811 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m10:29:07.451266 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m10:29:07.452054 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m10:29:07.480580 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m10:29:07.481260 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","None")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m10:29:07.482171 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Need to supply dataproc_region in profile to submit python job
[0m10:29:07.484726 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1533, in submit_python_job
    job_helper = self.python_submission_helpers[submission_method](
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 43, in __init__
    raise ValueError(
ValueError: Need to supply dataproc_region in profile to submit python job

[0m10:29:07.485945 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d448c5750>]}
[0m10:29:07.486531 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 0.07s]
[0m10:29:07.487152 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m10:29:07.487611 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m10:29:07.488136 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m10:29:07.489262 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m10:29:07.489614 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m10:29:07.492218 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m10:29:07.492809 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m10:29:07.495395 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:29:08.324936 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m10:29:08.325598 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m10:29:09.983430 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:07b9a7df-d807-487a-842a-f31177b6527d&page=queryresults
[0m10:29:12.147731 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5944c15d-a723-41cd-bed5-c3beaa32f324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d443429d0>]}
[0m10:29:12.148638 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.66s]
[0m10:29:12.149536 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m10:29:12.150211 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m10:29:12.150938 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m10:29:12.151617 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m10:29:12.152951 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:29:12.153376 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m10:29:12.153871 [info ] [MainThread]: 
[0m10:29:12.154343 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 6.66 seconds (6.66s).
[0m10:29:12.155433 [debug] [MainThread]: Command end result
[0m10:29:12.183010 [info ] [MainThread]: 
[0m10:29:12.183493 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m10:29:12.183844 [info ] [MainThread]: 
[0m10:29:12.184320 [error] [MainThread]:   Need to supply dataproc_region in profile to submit python job
[0m10:29:12.184669 [info ] [MainThread]: 
[0m10:29:12.185200 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m10:29:12.186727 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 7.745652, "process_user_time": 3.073269, "process_kernel_time": 0.259095, "process_mem_max_rss": "205124", "process_in_blocks": "21400", "process_out_blocks": "1984", "command_success": false}
[0m10:29:12.187494 [debug] [MainThread]: Command `dbt run` failed at 10:29:12.187408 after 7.75 seconds
[0m10:29:12.187877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d63245ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d668dae90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d5d6697d150>]}
[0m10:29:12.188250 [debug] [MainThread]: Flushing usage events
[0m10:36:37.302018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62b2d7a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62b2d7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62b2d7b90>]}


============================== 10:36:37.304087 | 8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2 ==============================
[0m10:36:37.304087 [info ] [MainThread]: Running with dbt=1.8.5
[0m10:36:37.304909 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:36:37.958026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62b2df9d0>]}
[0m10:36:37.997822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62d014c10>]}
[0m10:36:37.998552 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m10:36:38.005295 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m10:36:38.068160 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m10:36:38.068737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62c7d3d90>]}
[0m10:36:38.979705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a60c887b50>]}
[0m10:36:39.047399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a5ff644a50>]}
[0m10:36:39.047963 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m10:36:39.048363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a5ff76a850>]}
[0m10:36:39.049953 [info ] [MainThread]: 
[0m10:36:39.050613 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:36:39.053637 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m10:36:39.054269 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:36:40.064048 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m10:36:40.066423 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:36:40.778780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a60d357f10>]}
[0m10:36:40.779813 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:36:40.780518 [info ] [MainThread]: 
[0m10:36:40.784461 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m10:36:40.785518 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m10:36:40.786810 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m10:36:40.787830 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m10:36:40.823441 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m10:36:40.824409 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m10:36:40.853467 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m10:36:40.854292 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","None")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m10:36:40.854954 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Need to supply gcs_bucket in profile to submit python job
[0m10:36:40.857523 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1533, in submit_python_job
    job_helper = self.python_submission_helpers[submission_method](
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 43, in __init__
    raise ValueError(
ValueError: Need to supply gcs_bucket in profile to submit python job

[0m10:36:40.858870 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a60c9b0dd0>]}
[0m10:36:40.859470 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 0.07s]
[0m10:36:40.859996 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m10:36:40.860406 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m10:36:40.860830 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m10:36:40.861255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m10:36:40.861601 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m10:36:40.863985 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m10:36:40.865127 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m10:36:40.867060 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:36:41.514417 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m10:36:41.516314 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m10:36:43.142641 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:ffbedb48-b1ff-4178-a63c-2f2f14c7e9fe&page=queryresults
[0m10:36:45.531485 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e928c04-4e04-4a7c-a8b0-043fb6f9eeb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a60ca426d0>]}
[0m10:36:45.532605 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.67s]
[0m10:36:45.533496 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m10:36:45.534154 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m10:36:45.534871 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m10:36:45.535553 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m10:36:45.537148 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:36:45.537627 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m10:36:45.538188 [info ] [MainThread]: 
[0m10:36:45.538704 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 6.49 seconds (6.49s).
[0m10:36:45.539854 [debug] [MainThread]: Command end result
[0m10:36:45.569447 [info ] [MainThread]: 
[0m10:36:45.569927 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m10:36:45.570292 [info ] [MainThread]: 
[0m10:36:45.570697 [error] [MainThread]:   Need to supply gcs_bucket in profile to submit python job
[0m10:36:45.571018 [info ] [MainThread]: 
[0m10:36:45.571377 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m10:36:45.572117 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 8.30778, "process_user_time": 3.846648, "process_kernel_time": 0.219012, "process_mem_max_rss": "209332", "process_out_blocks": "2952", "command_success": false, "process_in_blocks": "0"}
[0m10:36:45.572601 [debug] [MainThread]: Command `dbt run` failed at 10:36:45.572517 after 8.31 seconds
[0m10:36:45.572980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62b2dd410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62ecf6e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a62b2bd710>]}
[0m10:36:45.573342 [debug] [MainThread]: Flushing usage events
[0m10:45:12.950748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73831f8476d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73831f847350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73831f8470d0>]}


============================== 10:45:12.952775 | 7220139c-9a75-4c58-ac34-0322c5f12f73 ==============================
[0m10:45:12.952775 [info ] [MainThread]: Running with dbt=1.8.5
[0m10:45:12.953423 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m10:45:13.594010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73830948c290>]}
[0m10:45:13.631256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x738321581410>]}
[0m10:45:13.631792 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m10:45:13.638246 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m10:45:13.701906 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m10:45:13.702445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x738320d48110>]}
[0m10:45:14.568337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73830104fc10>]}
[0m10:45:14.621948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7382f3b3c750>]}
[0m10:45:14.622376 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m10:45:14.622789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x738300ecf910>]}
[0m10:45:14.624171 [info ] [MainThread]: 
[0m10:45:14.624729 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:45:14.627578 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m10:45:14.628112 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:45:15.633267 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m10:45:15.634985 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:45:16.265059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x738301066910>]}
[0m10:45:16.265709 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:45:16.266183 [info ] [MainThread]: 
[0m10:45:16.268506 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m10:45:16.269093 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m10:45:16.269633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m10:45:16.270079 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m10:45:16.294541 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m10:45:16.295181 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m10:45:16.322980 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m10:45:16.323695 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m10:45:17.201469 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
403 GET https://storage.googleapis.com/storage/v1/b/quang-ltu-bucket?projection=noAcl&prettyPrint=false: python-youtube-sentiment@sentiment-analysis-410608.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).
[0m10:45:17.233360 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 73, in submit
    self._upload_to_gcs(self.model_file_name, compiled_code)
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 67, in _upload_to_gcs
    bucket = self.storage_client.get_bucket(self.credential.gcs_bucket)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/anaconda3/lib/python3.11/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/client.py", line 871, in get_bucket
    bucket.reload(
  File "/home/micasidad/anaconda3/lib/python3.11/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/bucket.py", line 1088, in reload
    super(Bucket, self).reload(
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/_helpers.py", line 300, in reload
    api_response = client._get_resource(
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/client.py", line 464, in _get_resource
    return self._connection.api_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/_http.py", line 90, in api_request
    return call()
           ^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 293, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 153, in retry_target
    _retry_error_helper(
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 144, in retry_target
    result = target()
             ^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/_http/__init__.py", line 494, in api_request
    raise exceptions.from_http_response(response)
google.api_core.exceptions.Forbidden: 403 GET https://storage.googleapis.com/storage/v1/b/quang-ltu-bucket?projection=noAcl&prettyPrint=false: python-youtube-sentiment@sentiment-analysis-410608.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).

[0m10:45:17.238602 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7382f3b80d50>]}
[0m10:45:17.240161 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 0.97s]
[0m10:45:17.241487 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m10:45:17.242330 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m10:45:17.244234 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m10:45:17.245892 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m10:45:17.246574 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m10:45:17.250361 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m10:45:17.251158 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m10:45:17.253723 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:45:17.949063 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m10:45:17.950798 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m10:45:19.659862 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:abee20f7-4b47-4517-9697-d9d943424b98&page=queryresults
[0m10:45:21.882086 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7220139c-9a75-4c58-ac34-0322c5f12f73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7382f3b3f050>]}
[0m10:45:21.883131 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.64s]
[0m10:45:21.884205 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m10:45:21.884849 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m10:45:21.885560 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m10:45:21.886230 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m10:45:21.887821 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:45:21.888350 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m10:45:21.889004 [info ] [MainThread]: 
[0m10:45:21.889513 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 7.26 seconds (7.26s).
[0m10:45:21.890787 [debug] [MainThread]: Command end result
[0m10:45:21.922247 [info ] [MainThread]: 
[0m10:45:21.922668 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m10:45:21.922976 [info ] [MainThread]: 
[0m10:45:21.923347 [error] [MainThread]:   403 GET https://storage.googleapis.com/storage/v1/b/quang-ltu-bucket?projection=noAcl&prettyPrint=false: python-youtube-sentiment@sentiment-analysis-410608.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).
[0m10:45:21.923663 [info ] [MainThread]: 
[0m10:45:21.923983 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m10:45:21.924679 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 9.010668, "process_user_time": 3.809034, "process_kernel_time": 0.236561, "process_mem_max_rss": "216036", "process_in_blocks": "320", "process_out_blocks": "2952", "command_success": false}
[0m10:45:21.925144 [debug] [MainThread]: Command `dbt run` failed at 10:45:21.925063 after 9.01 seconds
[0m10:45:21.925507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x738323216e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73831f87a010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73831f845750>]}
[0m10:45:21.925848 [debug] [MainThread]: Flushing usage events
[0m10:48:23.295198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca084bf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca084bbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca084bd10>]}


============================== 10:48:23.297269 | dcdf16f1-5d9d-4346-ab49-2bf815fc1419 ==============================
[0m10:48:23.297269 [info ] [MainThread]: Running with dbt=1.8.5
[0m10:48:23.298097 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:48:23.977752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca08b45d0>]}
[0m10:48:24.020996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca2568110>]}
[0m10:48:24.021575 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m10:48:24.028718 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m10:48:24.106423 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:48:24.106799 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:48:24.209909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bac7debdb10>]}
[0m10:48:24.280910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bac7deb0c10>]}
[0m10:48:24.281380 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m10:48:24.281770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bac7dedf610>]}
[0m10:48:24.283168 [info ] [MainThread]: 
[0m10:48:24.283845 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:48:24.286808 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m10:48:24.287366 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:48:25.320350 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m10:48:25.322567 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:48:25.951213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bac7e689090>]}
[0m10:48:25.953538 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:48:25.954098 [info ] [MainThread]: 
[0m10:48:25.956640 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m10:48:25.957300 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m10:48:25.957944 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m10:48:25.958484 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m10:48:25.987437 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m10:48:25.988224 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m10:48:26.014905 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m10:48:26.015599 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m10:48:26.853044 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
403 GET https://storage.googleapis.com/storage/v1/b/quang-ltu-bucket?projection=noAcl&prettyPrint=false: python-youtube-sentiment@sentiment-analysis-410608.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).
[0m10:48:26.882663 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 73, in submit
    self._upload_to_gcs(self.model_file_name, compiled_code)
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 67, in _upload_to_gcs
    bucket = self.storage_client.get_bucket(self.credential.gcs_bucket)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/anaconda3/lib/python3.11/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/client.py", line 871, in get_bucket
    bucket.reload(
  File "/home/micasidad/anaconda3/lib/python3.11/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/bucket.py", line 1088, in reload
    super(Bucket, self).reload(
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/_helpers.py", line 300, in reload
    api_response = client._get_resource(
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/client.py", line 464, in _get_resource
    return self._connection.api_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/storage/_http.py", line 90, in api_request
    return call()
           ^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 293, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 153, in retry_target
    _retry_error_helper(
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py", line 144, in retry_target
    result = target()
             ^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/_http/__init__.py", line 494, in api_request
    raise exceptions.from_http_response(response)
google.api_core.exceptions.Forbidden: 403 GET https://storage.googleapis.com/storage/v1/b/quang-ltu-bucket?projection=noAcl&prettyPrint=false: python-youtube-sentiment@sentiment-analysis-410608.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).

[0m10:48:26.889994 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bac7c475510>]}
[0m10:48:26.892052 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 0.93s]
[0m10:48:26.894092 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m10:48:26.895608 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m10:48:26.898027 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m10:48:26.898716 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m10:48:26.899130 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m10:48:26.901884 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m10:48:26.902457 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m10:48:26.907693 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:48:27.704534 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m10:48:27.706619 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m10:48:29.367990 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:a6f35d87-973b-492b-b868-31be78fdc7d9&page=queryresults
[0m10:48:31.403983 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dcdf16f1-5d9d-4346-ab49-2bf815fc1419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bac7dfb1790>]}
[0m10:48:31.404965 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.51s]
[0m10:48:31.405902 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m10:48:31.406589 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m10:48:31.407279 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m10:48:31.407972 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m10:48:31.409485 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:48:31.409991 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m10:48:31.410644 [info ] [MainThread]: 
[0m10:48:31.411211 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 7.13 seconds (7.13s).
[0m10:48:31.412308 [debug] [MainThread]: Command end result
[0m10:48:31.435137 [info ] [MainThread]: 
[0m10:48:31.435985 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m10:48:31.436437 [info ] [MainThread]: 
[0m10:48:31.436929 [error] [MainThread]:   403 GET https://storage.googleapis.com/storage/v1/b/quang-ltu-bucket?projection=noAcl&prettyPrint=false: python-youtube-sentiment@sentiment-analysis-410608.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).
[0m10:48:31.437259 [info ] [MainThread]: 
[0m10:48:31.437601 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m10:48:31.438437 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 8.179683, "process_user_time": 3.150668, "process_kernel_time": 0.264896, "process_mem_max_rss": "211568", "process_out_blocks": "2000", "command_success": false, "process_in_blocks": "0"}
[0m10:48:31.438922 [debug] [MainThread]: Command `dbt run` failed at 10:48:31.438840 after 8.18 seconds
[0m10:48:31.439386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca4216e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca0884d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7baca42b9910>]}
[0m10:48:31.439764 [debug] [MainThread]: Flushing usage events
[0m10:54:42.357700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab97674d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab9767d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab9767fd0>]}


============================== 10:54:42.359812 | 61aa50ef-b755-4931-aed2-02c1df9b9425 ==============================
[0m10:54:42.359812 [info ] [MainThread]: Running with dbt=1.8.5
[0m10:54:42.360448 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:54:43.001205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a973cc390>]}
[0m10:54:43.038324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abb4acfd0>]}
[0m10:54:43.038849 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m10:54:43.045498 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m10:54:43.122786 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:54:43.123139 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:54:43.223127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aba581dd0>]}
[0m10:54:43.287197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a96d50850>]}
[0m10:54:43.287682 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m10:54:43.288062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a97a7a110>]}
[0m10:54:43.289496 [info ] [MainThread]: 
[0m10:54:43.290050 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:54:43.292946 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m10:54:43.293308 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:54:44.418089 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m10:54:44.420279 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:54:45.100769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a96faed10>]}
[0m10:54:45.102982 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:54:45.104487 [info ] [MainThread]: 
[0m10:54:45.111815 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m10:54:45.113999 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m10:54:45.116065 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m10:54:45.117712 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m10:54:45.149168 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m10:54:45.149831 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m10:54:45.176561 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m10:54:45.177278 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m10:54:46.257937 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: fac4fd80-35c0-4cf5-8511-70cec42f238a
[0m10:54:46.871641 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
403 Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {
  description: "Google developers console API activation"
  url: "https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608"
}
, reason: "SERVICE_DISABLED"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "dataproc.googleapis.com"
}
metadata {
  key: "consumer"
  value: "projects/sentiment-analysis-410608"
}
]
[0m10:54:46.877200 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1181, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.PERMISSION_DENIED
	details = "Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry."
	debug_error_string = "UNKNOWN:Error received from peer ipv4:172.217.167.106:443 {created_time:"2024-08-18T10:54:46.870165595+10:00", grpc_status:7, grpc_message:"Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry."}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 143, in _submit_dataproc_job
    self.job_client.create_batch(request=request)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 838, in create_batch
    response = rpc(
               ^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.PermissionDenied: 403 Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {
  description: "Google developers console API activation"
  url: "https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608"
}
, reason: "SERVICE_DISABLED"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "dataproc.googleapis.com"
}
metadata {
  key: "consumer"
  value: "projects/sentiment-analysis-410608"
}
]

[0m10:54:46.880385 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a96ca9110>]}
[0m10:54:46.881572 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 1.76s]
[0m10:54:46.882862 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m10:54:46.883789 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m10:54:46.885478 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m10:54:46.886534 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m10:54:46.887412 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m10:54:46.893491 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m10:54:46.894373 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m10:54:46.897254 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:54:47.607101 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m10:54:47.608805 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m10:54:49.228390 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:0d296dd7-d3d9-42b2-9b35-deffb21182b6&page=queryresults
[0m10:54:51.306182 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61aa50ef-b755-4931-aed2-02c1df9b9425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a96f60f90>]}
[0m10:54:51.307092 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.42s]
[0m10:54:51.307944 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m10:54:51.308552 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m10:54:51.309231 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m10:54:51.309877 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m10:54:51.311426 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:54:51.311899 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m10:54:51.312433 [info ] [MainThread]: 
[0m10:54:51.312956 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 8.02 seconds (8.02s).
[0m10:54:51.314217 [debug] [MainThread]: Command end result
[0m10:54:51.343765 [info ] [MainThread]: 
[0m10:54:51.344193 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m10:54:51.344501 [info ] [MainThread]: 
[0m10:54:51.344884 [error] [MainThread]:   403 Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {
  description: "Google developers console API activation"
  url: "https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608"
}
, reason: "SERVICE_DISABLED"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "dataproc.googleapis.com"
}
metadata {
  key: "consumer"
  value: "projects/sentiment-analysis-410608"
}
]
[0m10:54:51.345217 [info ] [MainThread]: 
[0m10:54:51.345537 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m10:54:51.346251 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 9.025162, "process_user_time": 3.123201, "process_kernel_time": 0.218037, "process_mem_max_rss": "213232", "process_out_blocks": "2000", "command_success": false, "process_in_blocks": "0"}
[0m10:54:51.346706 [debug] [MainThread]: Command `dbt run` failed at 10:54:51.346626 after 9.03 seconds
[0m10:54:51.347066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abd162e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab976d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abac5e850>]}
[0m10:54:51.347414 [debug] [MainThread]: Flushing usage events
[0m11:01:26.602941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903d81abb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903d81abcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903d81abad0>]}


============================== 11:01:26.604990 | 03392ef8-8278-4694-8da1-8b56296c0a39 ==============================
[0m11:01:26.604990 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:01:26.605737 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:01:27.297946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903bc2c8d50>]}
[0m11:01:27.343574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903d9f10f10>]}
[0m11:01:27.344173 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:01:27.352320 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:01:27.433043 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:01:27.433411 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:01:27.532256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903d96e9990>]}
[0m11:01:27.602501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903b58a7490>]}
[0m11:01:27.602955 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m11:01:27.603387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903b59e9c10>]}
[0m11:01:27.604729 [info ] [MainThread]: 
[0m11:01:27.605372 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:01:27.608649 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:01:27.609339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:01:28.659089 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:01:28.660742 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:01:29.377971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903b5b0e110>]}
[0m11:01:29.380336 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:01:29.380902 [info ] [MainThread]: 
[0m11:01:29.383395 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:01:29.384026 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:01:29.384689 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:01:29.385172 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:01:29.419547 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:01:29.420207 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:01:29.446447 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:01:29.447155 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:01:30.079129 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: a6e074fd-c9da-44f1-ae36-512f4c76674e
[0m11:01:31.024289 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
403 Permission 'dataproc.batches.create' denied on resource '//dataproc.googleapis.com/projects/sentiment-analysis-410608/locations/australia-southeast1' (or it may not exist). [reason: "IAM_PERMISSION_DENIED"
domain: "dataproc.googleapis.com"
metadata {
  key: "resource"
  value: "projects/sentiment-analysis-410608/locations/australia-southeast1"
}
metadata {
  key: "permission"
  value: "dataproc.batches.create"
}
]
[0m11:01:31.030511 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1181, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.PERMISSION_DENIED
	details = "Permission 'dataproc.batches.create' denied on resource '//dataproc.googleapis.com/projects/sentiment-analysis-410608/locations/australia-southeast1' (or it may not exist)."
	debug_error_string = "UNKNOWN:Error received from peer ipv4:172.217.167.74:443 {grpc_message:"Permission \'dataproc.batches.create\' denied on resource \'//dataproc.googleapis.com/projects/sentiment-analysis-410608/locations/australia-southeast1\' (or it may not exist).", grpc_status:7, created_time:"2024-08-18T11:01:31.02018895+10:00"}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 143, in _submit_dataproc_job
    self.job_client.create_batch(request=request)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 838, in create_batch
    response = rpc(
               ^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.PermissionDenied: 403 Permission 'dataproc.batches.create' denied on resource '//dataproc.googleapis.com/projects/sentiment-analysis-410608/locations/australia-southeast1' (or it may not exist). [reason: "IAM_PERMISSION_DENIED"
domain: "dataproc.googleapis.com"
metadata {
  key: "resource"
  value: "projects/sentiment-analysis-410608/locations/australia-southeast1"
}
metadata {
  key: "permission"
  value: "dataproc.batches.create"
}
]

[0m11:01:31.033524 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903b5af7390>]}
[0m11:01:31.034373 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 1.65s]
[0m11:01:31.035163 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:01:31.035752 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:01:31.037020 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:01:31.037788 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:01:31.038373 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:01:31.041940 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:01:31.042785 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:01:31.045460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:01:31.770754 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:01:31.773193 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:01:33.183245 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:339cffa5-db2b-4885-b30c-48dc2f87679d&page=queryresults
[0m11:01:35.627004 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03392ef8-8278-4694-8da1-8b56296c0a39', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903d81b21d0>]}
[0m11:01:35.627723 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.59s]
[0m11:01:35.628448 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:01:35.628942 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:01:35.629413 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:01:35.629996 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:01:35.631116 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:01:35.631478 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:01:35.631925 [info ] [MainThread]: 
[0m11:01:35.632357 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 8.03 seconds (8.03s).
[0m11:01:35.633270 [debug] [MainThread]: Command end result
[0m11:01:35.664367 [info ] [MainThread]: 
[0m11:01:35.664780 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:01:35.665096 [info ] [MainThread]: 
[0m11:01:35.665538 [error] [MainThread]:   403 Permission 'dataproc.batches.create' denied on resource '//dataproc.googleapis.com/projects/sentiment-analysis-410608/locations/australia-southeast1' (or it may not exist). [reason: "IAM_PERMISSION_DENIED"
domain: "dataproc.googleapis.com"
metadata {
  key: "resource"
  value: "projects/sentiment-analysis-410608/locations/australia-southeast1"
}
metadata {
  key: "permission"
  value: "dataproc.batches.create"
}
]
[0m11:01:35.666096 [info ] [MainThread]: 
[0m11:01:35.666467 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:01:35.667384 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 9.103192, "process_user_time": 3.043955, "process_kernel_time": 0.235208, "process_mem_max_rss": "212680", "process_out_blocks": "1992", "command_success": false, "process_in_blocks": "0"}
[0m11:01:35.667852 [debug] [MainThread]: Command `dbt run` failed at 11:01:35.667768 after 9.10 seconds
[0m11:01:35.668261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903dbbf6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903dbc99a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7903dbc997d0>]}
[0m11:01:35.668918 [debug] [MainThread]: Flushing usage events
[0m11:03:00.186645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71157c86e190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71157c86fdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71157c837950>]}


============================== 11:03:00.188808 | aa817621-900e-40c2-8788-bb3c41e2a742 ==============================
[0m11:03:00.188808 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:03:00.189321 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:03:00.830121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71157d650250>]}
[0m11:03:00.867256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71157e574d50>]}
[0m11:03:00.867746 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:03:00.874339 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:03:00.951350 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:03:00.951711 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:03:01.051403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71157ddb0110>]}
[0m11:03:01.115593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71155a84fcd0>]}
[0m11:03:01.116084 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m11:03:01.116465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71155a125c10>]}
[0m11:03:01.117908 [info ] [MainThread]: 
[0m11:03:01.118523 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:03:01.121403 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:03:01.121897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:03:02.157211 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:03:02.159408 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:03:02.875356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71155a84c350>]}
[0m11:03:02.877672 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:03:02.878665 [info ] [MainThread]: 
[0m11:03:02.881096 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:03:02.881756 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:03:02.882409 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:03:02.882905 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:03:02.911480 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:03:02.912099 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:03:02.938586 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:03:02.939264 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:03:03.593341 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 4c48d0e7-9000-494b-9f5b-a63536bd2de3
[0m11:03:04.098677 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
403 Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {
  description: "Google developers console API activation"
  url: "https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608"
}
, reason: "SERVICE_DISABLED"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "dataproc.googleapis.com"
}
metadata {
  key: "consumer"
  value: "projects/sentiment-analysis-410608"
}
]
[0m11:03:04.109572 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1181, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.PERMISSION_DENIED
	details = "Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry."
	debug_error_string = "UNKNOWN:Error received from peer ipv4:172.217.167.106:443 {created_time:"2024-08-18T11:03:04.09638836+10:00", grpc_status:7, grpc_message:"Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry."}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 143, in _submit_dataproc_job
    self.job_client.create_batch(request=request)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 838, in create_batch
    response = rpc(
               ^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.PermissionDenied: 403 Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {
  description: "Google developers console API activation"
  url: "https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608"
}
, reason: "SERVICE_DISABLED"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "dataproc.googleapis.com"
}
metadata {
  key: "consumer"
  value: "projects/sentiment-analysis-410608"
}
]

[0m11:03:04.116915 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71157dcfe8d0>]}
[0m11:03:04.119097 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 1.23s]
[0m11:03:04.121186 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:03:04.122867 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:03:04.125034 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:03:04.130166 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:03:04.131575 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:03:04.141272 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:03:04.143539 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:03:04.149129 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:03:04.800120 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:03:04.802307 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:03:06.455588 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:90d8ef06-90f2-4a90-a0cb-8ac26924e9a6&page=queryresults
[0m11:03:08.417243 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa817621-900e-40c2-8788-bb3c41e2a742', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x711559e50d90>]}
[0m11:03:08.418529 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.29s]
[0m11:03:08.419784 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:03:08.420584 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:03:08.421471 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:03:08.422429 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:03:08.424190 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:03:08.424834 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:03:08.425605 [info ] [MainThread]: 
[0m11:03:08.426762 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 7.31 seconds (7.31s).
[0m11:03:08.429040 [debug] [MainThread]: Command end result
[0m11:03:08.460193 [info ] [MainThread]: 
[0m11:03:08.460689 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:03:08.461072 [info ] [MainThread]: 
[0m11:03:08.461513 [error] [MainThread]:   403 Cloud Dataproc API has not been used in project sentiment-analysis-410608 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {
  description: "Google developers console API activation"
  url: "https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview?project=sentiment-analysis-410608"
}
, reason: "SERVICE_DISABLED"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "dataproc.googleapis.com"
}
metadata {
  key: "consumer"
  value: "projects/sentiment-analysis-410608"
}
]
[0m11:03:08.461870 [info ] [MainThread]: 
[0m11:03:08.462273 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:03:08.463014 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 8.313333, "process_user_time": 3.185408, "process_kernel_time": 0.212947, "process_mem_max_rss": "212992", "process_out_blocks": "2000", "command_success": false, "process_in_blocks": "0"}
[0m11:03:08.463460 [debug] [MainThread]: Command `dbt run` failed at 11:03:08.463380 after 8.31 seconds
[0m11:03:08.463812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x711580216e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7115802b9010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7115802b9910>]}
[0m11:03:08.464149 [debug] [MainThread]: Flushing usage events
[0m11:07:09.104888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ae9a8f810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ae9ad2ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ae9ad2850>]}


============================== 11:07:09.106929 | 1f52c554-7457-4eb6-9a21-322d410006ce ==============================
[0m11:07:09.106929 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:07:09.107654 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:07:09.742532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ac737b9d0>]}
[0m11:07:09.779522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756aeb7dcd10>]}
[0m11:07:09.780025 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:07:09.786438 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:07:09.862576 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:07:09.862922 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:07:09.960411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ac7148110>]}
[0m11:07:10.027160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756aeb855890>]}
[0m11:07:10.027625 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m11:07:10.028037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ac711f290>]}
[0m11:07:10.029469 [info ] [MainThread]: 
[0m11:07:10.030093 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:07:10.032942 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:07:10.033472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:07:11.085383 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:07:11.087294 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:07:11.771968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756aeb4dca10>]}
[0m11:07:11.773458 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:07:11.774162 [info ] [MainThread]: 
[0m11:07:11.777279 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:07:11.778088 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:07:11.778859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:07:11.779441 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:07:11.818617 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:07:11.819347 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:07:11.850204 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:07:11.850992 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:07:12.520517 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 83c7ebaf-bb41-42be-9359-37b10d7adbae
[0m11:07:14.053580 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
400 User not authorized to act as service account '906804028770-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.
[0m11:07:14.071378 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1181, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = "User not authorized to act as service account '906804028770-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details."
	debug_error_string = "UNKNOWN:Error received from peer ipv4:142.250.71.74:443 {grpc_message:"User not authorized to act as service account \'906804028770-compute@developer.gserviceaccount.com\'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.", grpc_status:3, created_time:"2024-08-18T11:07:14.04971254+10:00"}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 143, in _submit_dataproc_job
    self.job_client.create_batch(request=request)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 838, in create_batch
    response = rpc(
               ^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InvalidArgument: 400 User not authorized to act as service account '906804028770-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.

[0m11:07:14.080233 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ac7303890>]}
[0m11:07:14.084166 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 2.30s]
[0m11:07:14.087034 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:07:14.088947 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:07:14.094314 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:07:14.096476 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:07:14.097666 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:07:14.106152 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:07:14.107763 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:07:14.113940 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:14.780112 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:07:14.781823 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:07:15.178487 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:2df65b45-bf63-41f8-a6c1-de6efbc6279b&page=queryresults
[0m11:07:17.779584 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1f52c554-7457-4eb6-9a21-322d410006ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756ae9ae6190>]}
[0m11:07:17.780642 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.68s]
[0m11:07:17.781703 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:07:17.782428 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:07:17.783172 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:07:17.783949 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:07:17.785508 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:07:17.786059 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:07:17.786697 [info ] [MainThread]: 
[0m11:07:17.787397 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 7.76 seconds (7.76s).
[0m11:07:17.788740 [debug] [MainThread]: Command end result
[0m11:07:17.815328 [info ] [MainThread]: 
[0m11:07:17.815769 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:07:17.816082 [info ] [MainThread]: 
[0m11:07:17.816453 [error] [MainThread]:   400 User not authorized to act as service account '906804028770-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.
[0m11:07:17.816781 [info ] [MainThread]: 
[0m11:07:17.817140 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:07:17.817871 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 8.748964, "process_user_time": 3.135168, "process_kernel_time": 0.221069, "process_mem_max_rss": "212652", "process_out_blocks": "2000", "command_success": false, "process_in_blocks": "0"}
[0m11:07:17.818350 [debug] [MainThread]: Command `dbt run` failed at 11:07:17.818268 after 8.75 seconds
[0m11:07:17.818729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756aed4bae50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756aed55da50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756aed55d7d0>]}
[0m11:07:17.819097 [debug] [MainThread]: Flushing usage events
[0m11:08:06.244275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723417e279d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723417e5e090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723417e5f390>]}


============================== 11:08:06.246391 | e386225a-ba1f-4c82-9467-cafba6293bc7 ==============================
[0m11:08:06.246391 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:08:06.247114 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:08:06.882799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7233f5e687d0>]}
[0m11:08:06.919914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723419b6d150>]}
[0m11:08:06.920415 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:08:06.927026 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:08:07.004396 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:08:07.004752 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:08:07.103583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72341a543750>]}
[0m11:08:07.166263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7233f5c1b450>]}
[0m11:08:07.166710 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m11:08:07.167122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7234184c5fd0>]}
[0m11:08:07.168465 [info ] [MainThread]: 
[0m11:08:07.169084 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:08:07.171970 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:08:07.172422 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:08:08.120936 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:08:08.122883 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:08:08.699563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7234184c5fd0>]}
[0m11:08:08.702304 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:08:08.704023 [info ] [MainThread]: 
[0m11:08:08.712323 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:08:08.714787 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:08:08.717210 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:08:08.719320 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:08:08.781042 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:08:08.782252 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:08:08.816164 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:08:08.816835 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:08:09.448679 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 9cb3091b-a90e-46f5-83e3-b4e8f6f250e9
[0m11:08:15.251433 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
400 Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.
[0m11:08:15.267647 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1181, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/grpc/_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = "Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'."
	debug_error_string = "UNKNOWN:Error received from peer ipv4:142.250.71.74:443 {created_time:"2024-08-18T11:08:15.247110525+10:00", grpc_status:3, grpc_message:"Subnetwork \'default\' does not support Private Google Access which is required for Dataproc clusters when \'internal_ip_only\' is set to \'true\'. Enable Private Google Access on subnetwork \'default\' or set \'internal_ip_only\' to \'false\'."}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 143, in _submit_dataproc_job
    self.job_client.create_batch(request=request)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 838, in create_batch
    response = rpc(
               ^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InvalidArgument: 400 Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.

[0m11:08:15.270830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723418485f10>]}
[0m11:08:15.272395 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 6.55s]
[0m11:08:15.273772 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:08:15.274804 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:08:15.276849 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:08:15.277800 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:08:15.278640 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:08:15.284438 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:08:15.285342 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:08:15.288451 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:16.010960 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:08:16.012702 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:08:17.641630 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:f8d4968c-4097-4e48-b613-a7f0b05ba04f&page=queryresults
[0m11:08:19.674700 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e386225a-ba1f-4c82-9467-cafba6293bc7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723418a97dd0>]}
[0m11:08:19.675752 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.40s]
[0m11:08:19.676733 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:08:19.677383 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:08:19.678072 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:08:19.678730 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:08:19.680335 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:08:19.680826 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:08:19.681392 [info ] [MainThread]: 
[0m11:08:19.681848 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 12.51 seconds (12.51s).
[0m11:08:19.683330 [debug] [MainThread]: Command end result
[0m11:08:19.711531 [info ] [MainThread]: 
[0m11:08:19.712314 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:08:19.712847 [info ] [MainThread]: 
[0m11:08:19.713558 [error] [MainThread]:   400 Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.
[0m11:08:19.714097 [info ] [MainThread]: 
[0m11:08:19.714681 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:08:19.715759 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 13.50764, "process_user_time": 3.198042, "process_kernel_time": 0.2186, "process_mem_max_rss": "213476", "process_out_blocks": "2000", "command_success": false, "process_in_blocks": "0"}
[0m11:08:19.716516 [debug] [MainThread]: Command `dbt run` failed at 11:08:19.716414 after 13.51 seconds
[0m11:08:19.717035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72341b816e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72341b8b9a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72341b8b97d0>]}
[0m11:08:19.717489 [debug] [MainThread]: Flushing usage events
[0m11:10:39.819212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd507b690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd50aafd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd50aa590>]}


============================== 11:10:39.821270 | f90417e4-5254-4d10-bebd-d9abd183f94c ==============================
[0m11:10:39.821270 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:10:39.821935 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:10:40.516421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd0f5a790>]}
[0m11:10:40.558453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd6db13d0>]}
[0m11:10:40.559070 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:10:40.566005 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:10:40.644025 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:10:40.644439 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:10:40.744351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd5081890>]}
[0m11:10:40.806885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bb26ca310>]}
[0m11:10:40.807335 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m11:10:40.807706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bb26b2590>]}
[0m11:10:40.809081 [info ] [MainThread]: 
[0m11:10:40.809646 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:10:40.812646 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:10:40.813168 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:10:41.716927 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:10:41.718870 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:10:42.334014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bb86ab990>]}
[0m11:10:42.338632 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:10:42.341549 [info ] [MainThread]: 
[0m11:10:42.350138 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:10:42.352942 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:10:42.355465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:10:42.357432 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:10:42.407870 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:10:42.408525 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:10:42.434842 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:10:42.435543 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd
from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:10:43.156878 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 9796e861-047e-421a-a79d-85b8584c22b4
[0m11:13:49.851606 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'scripts']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/9796e861-047e-421a-a79d-85b8584c22b4?project=sentiment-analysis-410608
gcloud dataproc batches wait '9796e861-047e-421a-a79d-85b8584c22b4' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f39502d2-9850-4610-9b7d-376b079c83e0/jobs/srvls-batch-980e17df-c654-43c4-855c-48666c5e78a3/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f39502d2-9850-4610-9b7d-376b079c83e0/jobs/srvls-batch-980e17df-c654-43c4-855c-48666c5e78a3/driveroutput.*
[0m11:13:49.857284 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'scripts']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/9796e861-047e-421a-a79d-85b8584c22b4?project=sentiment-analysis-410608
gcloud dataproc batches wait '9796e861-047e-421a-a79d-85b8584c22b4' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f39502d2-9850-4610-9b7d-376b079c83e0/jobs/srvls-batch-980e17df-c654-43c4-855c-48666c5e78a3/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f39502d2-9850-4610-9b7d-376b079c83e0/jobs/srvls-batch-980e17df-c654-43c4-855c-48666c5e78a3/driveroutput.*

[0m11:13:49.861817 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd54bb610>]}
[0m11:13:49.863186 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 187.51s]
[0m11:13:49.864500 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:13:49.865412 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:13:49.867289 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:13:49.868424 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:13:49.869419 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:13:49.874793 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:13:49.875906 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:13:49.883050 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:13:50.680381 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:13:50.682088 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:13:52.287541 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:5200e726-1c7a-4a69-8ea6-5ebad6da830a&page=queryresults
[0m11:13:54.584769 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f90417e4-5254-4d10-bebd-d9abd183f94c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bb293a1d0>]}
[0m11:13:54.585822 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.72s]
[0m11:13:54.586874 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:13:54.587551 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:13:54.588266 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:13:54.588922 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:13:54.590355 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:13:54.590861 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:13:54.591430 [info ] [MainThread]: 
[0m11:13:54.591971 [info ] [MainThread]: Finished running 3 table models in 0 hours 3 minutes and 13.78 seconds (193.78s).
[0m11:13:54.593172 [debug] [MainThread]: Command end result
[0m11:13:54.621467 [info ] [MainThread]: 
[0m11:13:54.622121 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:13:54.622598 [info ] [MainThread]: 
[0m11:13:54.623518 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'scripts']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/9796e861-047e-421a-a79d-85b8584c22b4?project=sentiment-analysis-410608
gcloud dataproc batches wait '9796e861-047e-421a-a79d-85b8584c22b4' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f39502d2-9850-4610-9b7d-376b079c83e0/jobs/srvls-batch-980e17df-c654-43c4-855c-48666c5e78a3/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f39502d2-9850-4610-9b7d-376b079c83e0/jobs/srvls-batch-980e17df-c654-43c4-855c-48666c5e78a3/driveroutput.*
[0m11:13:54.624475 [info ] [MainThread]: 
[0m11:13:54.625228 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:13:54.626469 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 194.8437, "process_user_time": 3.852797, "process_kernel_time": 0.443031, "process_mem_max_rss": "212536", "process_out_blocks": "2000", "command_success": false, "process_in_blocks": "0"}
[0m11:13:54.627363 [debug] [MainThread]: Command `dbt run` failed at 11:13:54.627255 after 194.84 seconds
[0m11:13:54.628048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd8a16e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd50aaed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741bd50ab910>]}
[0m11:13:54.628730 [debug] [MainThread]: Flushing usage events
[0m11:33:35.550219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c35487e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c35487d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c35487ad0>]}


============================== 11:33:35.552365 | 8783a20f-02ac-4eeb-8023-e57bdfcf51f8 ==============================
[0m11:33:35.552365 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:33:35.552993 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:33:36.236193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c12fcd150>]}
[0m11:33:36.278136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c371bd790>]}
[0m11:33:36.278731 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:33:36.285508 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:33:36.401827 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:33:36.402479 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m11:33:36.683664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c12c30950>]}
[0m11:33:36.752657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c12a4c090>]}
[0m11:33:36.753133 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m11:33:36.753574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c12c32c50>]}
[0m11:33:36.755036 [info ] [MainThread]: 
[0m11:33:36.755696 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:33:36.758683 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:33:36.759320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:38.835941 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:33:38.838068 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:33:39.551033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c12c01f10>]}
[0m11:33:39.552143 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:33:39.553090 [info ] [MainThread]: 
[0m11:33:39.559314 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:33:39.560798 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:33:39.562114 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:33:39.563158 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:33:39.599226 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:33:39.599953 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:33:39.627965 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:33:39.628666 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

# Add the scripts directory to the sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../scripts')))

from preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:33:40.377263 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 34792ada-5b1d-4a94-b004-69cb0358eef8
[0m11:36:33.276344 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'preprocess_data']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/34792ada-5b1d-4a94-b004-69cb0358eef8?project=sentiment-analysis-410608
gcloud dataproc batches wait '34792ada-5b1d-4a94-b004-69cb0358eef8' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/9adfdf94-4303-405e-8e4b-a8b4fdced9e9/jobs/srvls-batch-7689243b-6b93-424c-abd1-a6634cee58ae/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/9adfdf94-4303-405e-8e4b-a8b4fdced9e9/jobs/srvls-batch-7689243b-6b93-424c-abd1-a6634cee58ae/driveroutput.*
[0m11:36:33.283353 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'preprocess_data']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/34792ada-5b1d-4a94-b004-69cb0358eef8?project=sentiment-analysis-410608
gcloud dataproc batches wait '34792ada-5b1d-4a94-b004-69cb0358eef8' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/9adfdf94-4303-405e-8e4b-a8b4fdced9e9/jobs/srvls-batch-7689243b-6b93-424c-abd1-a6634cee58ae/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/9adfdf94-4303-405e-8e4b-a8b4fdced9e9/jobs/srvls-batch-7689243b-6b93-424c-abd1-a6634cee58ae/driveroutput.*

[0m11:36:33.286778 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c12c0b7d0>]}
[0m11:36:33.287708 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 173.72s]
[0m11:36:33.288648 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:36:33.289265 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:36:33.290701 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:36:33.291629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:36:33.292532 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:36:33.298138 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:36:33.299242 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:36:33.303459 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:36:34.275063 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:36:34.276198 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:36:35.884773 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:313c57af-e09f-4406-802a-ba36d518e743&page=queryresults
[0m11:36:38.078920 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8783a20f-02ac-4eeb-8023-e57bdfcf51f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c128e3050>]}
[0m11:36:38.079880 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.79s]
[0m11:36:38.080772 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:36:38.081427 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:36:38.082140 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:36:38.082780 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:36:38.084193 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:36:38.084628 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:36:38.085124 [info ] [MainThread]: 
[0m11:36:38.085589 [info ] [MainThread]: Finished running 3 table models in 0 hours 3 minutes and 1.33 seconds (181.33s).
[0m11:36:38.086753 [debug] [MainThread]: Command end result
[0m11:36:38.112466 [info ] [MainThread]: 
[0m11:36:38.113053 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:36:38.113685 [info ] [MainThread]: 
[0m11:36:38.114468 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'preprocess_data']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/34792ada-5b1d-4a94-b004-69cb0358eef8?project=sentiment-analysis-410608
gcloud dataproc batches wait '34792ada-5b1d-4a94-b004-69cb0358eef8' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/9adfdf94-4303-405e-8e4b-a8b4fdced9e9/jobs/srvls-batch-7689243b-6b93-424c-abd1-a6634cee58ae/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/9adfdf94-4303-405e-8e4b-a8b4fdced9e9/jobs/srvls-batch-7689243b-6b93-424c-abd1-a6634cee58ae/driveroutput.*
[0m11:36:38.115345 [info ] [MainThread]: 
[0m11:36:38.116148 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:36:38.117486 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 182.60944, "process_user_time": 3.996832, "process_kernel_time": 0.507727, "process_mem_max_rss": "214808", "process_out_blocks": "2952", "command_success": false, "process_in_blocks": "0"}
[0m11:36:38.118249 [debug] [MainThread]: Command `dbt run` failed at 11:36:38.118159 after 182.61 seconds
[0m11:36:38.118643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c38e16e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c35486090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5c35485b50>]}
[0m11:36:38.119273 [debug] [MainThread]: Flushing usage events
[0m11:38:01.327494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71483193fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71483193c5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71483193ff90>]}


============================== 11:38:01.331223 | 65a91209-0662-4c29-81bc-36838c0e4a98 ==============================
[0m11:38:01.331223 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:38:01.332425 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:38:02.059448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7148146b3e50>]}
[0m11:38:02.096516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714833670410>]}
[0m11:38:02.097044 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:38:02.103700 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:38:02.180389 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m11:38:02.180999 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/__init__.py
[0m11:38:02.181424 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m11:38:02.426496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71480f02f1d0>]}
[0m11:38:02.482552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71480eee4150>]}
[0m11:38:02.483028 [info ] [MainThread]: Found 4 models, 2 sources, 473 macros
[0m11:38:02.483385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71480f1171d0>]}
[0m11:38:02.484718 [info ] [MainThread]: 
[0m11:38:02.485238 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:38:02.488104 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:38:02.488582 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:03.423789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:38:03.424927 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:38:04.089377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71480f190750>]}
[0m11:38:04.089965 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:38:04.090353 [info ] [MainThread]: 
[0m11:38:04.092656 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:38:04.093461 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:38:04.094191 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:38:04.094864 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:38:04.120763 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:38:04.121505 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:38:04.149318 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:38:04.150043 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

# Add the scripts directory to the sys.path
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../scripts')))
# from preprocess_data import clean_text, get_sentiment_score

from scripts.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:38:04.900145 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 42992344-7e3f-4024-a6ae-de27a574ea76
[0m11:40:55.962155 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'scripts']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/42992344-7e3f-4024-a6ae-de27a574ea76?project=sentiment-analysis-410608
gcloud dataproc batches wait '42992344-7e3f-4024-a6ae-de27a574ea76' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ee213eab-2f7f-4ab5-87a5-c76132cbd6c1/jobs/srvls-batch-4b4e78d7-a650-434d-81b8-e71a99fa1b08/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ee213eab-2f7f-4ab5-87a5-c76132cbd6c1/jobs/srvls-batch-4b4e78d7-a650-434d-81b8-e71a99fa1b08/driveroutput.*
[0m11:40:55.968904 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'scripts']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/42992344-7e3f-4024-a6ae-de27a574ea76?project=sentiment-analysis-410608
gcloud dataproc batches wait '42992344-7e3f-4024-a6ae-de27a574ea76' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ee213eab-2f7f-4ab5-87a5-c76132cbd6c1/jobs/srvls-batch-4b4e78d7-a650-434d-81b8-e71a99fa1b08/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ee213eab-2f7f-4ab5-87a5-c76132cbd6c1/jobs/srvls-batch-4b4e78d7-a650-434d-81b8-e71a99fa1b08/driveroutput.*

[0m11:40:55.972431 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71480eddbe90>]}
[0m11:40:55.973844 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 171.88s]
[0m11:40:55.975325 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:40:55.976327 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:40:55.978371 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:40:55.979590 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:40:55.980595 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:40:55.984435 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:40:55.985312 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:40:55.988194 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:40:56.905594 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:40:56.906396 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:40:58.326686 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:b41308a6-7d62-4c20-bd6c-d1fd6bccc2df&page=queryresults
[0m11:41:00.944426 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65a91209-0662-4c29-81bc-36838c0e4a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71480eea5310>]}
[0m11:41:00.945450 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.96s]
[0m11:41:00.946379 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:41:00.947013 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:41:00.947723 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:41:00.948417 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:41:00.949904 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:41:00.950389 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:41:00.950877 [info ] [MainThread]: 
[0m11:41:00.951340 [info ] [MainThread]: Finished running 3 table models in 0 hours 2 minutes and 58.47 seconds (178.47s).
[0m11:41:00.952411 [debug] [MainThread]: Command end result
[0m11:41:00.979676 [info ] [MainThread]: 
[0m11:41:00.980283 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:41:00.980639 [info ] [MainThread]: 
[0m11:41:00.981268 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'scripts']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/42992344-7e3f-4024-a6ae-de27a574ea76?project=sentiment-analysis-410608
gcloud dataproc batches wait '42992344-7e3f-4024-a6ae-de27a574ea76' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ee213eab-2f7f-4ab5-87a5-c76132cbd6c1/jobs/srvls-batch-4b4e78d7-a650-434d-81b8-e71a99fa1b08/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ee213eab-2f7f-4ab5-87a5-c76132cbd6c1/jobs/srvls-batch-4b4e78d7-a650-434d-81b8-e71a99fa1b08/driveroutput.*
[0m11:41:00.981648 [info ] [MainThread]: 
[0m11:41:00.982119 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:41:00.982931 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 179.72351, "process_user_time": 3.937164, "process_kernel_time": 0.410812, "process_mem_max_rss": "214796", "process_out_blocks": "2976", "command_success": false, "process_in_blocks": "0"}
[0m11:41:00.983422 [debug] [MainThread]: Command `dbt run` failed at 11:41:00.983340 after 179.72 seconds
[0m11:41:00.983838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714835316e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71480f8df4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7148332d5e10>]}
[0m11:41:00.984524 [debug] [MainThread]: Flushing usage events
[0m11:51:54.557655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae2d577e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae2d5ae8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae2d5ae610>]}


============================== 11:51:54.559769 | d20cb266-cbd2-468e-8a1b-fc0dbc954b16 ==============================
[0m11:51:54.559769 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:51:54.560388 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:51:55.201263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd20cb266-cbd2-468e-8a1b-fc0dbc954b16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae0adacdd0>]}
[0m11:51:55.238537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd20cb266-cbd2-468e-8a1b-fc0dbc954b16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae2f2b8e10>]}
[0m11:51:55.239060 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:51:55.245707 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:51:55.324423 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m11:51:55.324905 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/test.py
[0m11:51:55.325296 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m11:51:55.519083 [error] [MainThread]: Encountered an error:
Parsing Error in model test (models/test.py)
  dbt allows exactly one model defined per python file, found 0
[0m11:51:55.519964 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.9982978, "process_user_time": 2.659264, "process_kernel_time": 0.172992, "process_mem_max_rss": "203124", "process_out_blocks": "8", "command_success": false, "process_in_blocks": "0"}
[0m11:51:55.520424 [debug] [MainThread]: Command `dbt run` failed at 11:51:55.520346 after 1.00 seconds
[0m11:51:55.520772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae2d577790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae30f82e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cae0acb5210>]}
[0m11:51:55.521122 [debug] [MainThread]: Flushing usage events
[0m11:52:25.755167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72daaec3bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72daaec3bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72daaec3b9d0>]}


============================== 11:52:25.757317 | b680f256-d486-4d66-a074-b22454e89fdd ==============================
[0m11:52:25.757317 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:52:25.758138 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:52:26.445783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da90529690>]}
[0m11:52:26.483631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72dab0959490>]}
[0m11:52:26.484194 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:52:26.490958 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:52:26.569124 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:52:26.569645 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m11:52:26.811726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da901f9510>]}
[0m11:52:26.869350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da901317d0>]}
[0m11:52:26.869807 [info ] [MainThread]: Found 4 models, 2 sources, 473 macros
[0m11:52:26.870201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da9009f1d0>]}
[0m11:52:26.871538 [info ] [MainThread]: 
[0m11:52:26.872109 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:52:26.875053 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:52:26.875452 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:27.867355 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:52:27.869607 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:52:28.513292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da90530a50>]}
[0m11:52:28.515834 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:52:28.517480 [info ] [MainThread]: 
[0m11:52:28.528841 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:52:28.531630 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:52:28.533978 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:52:28.535967 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:52:28.582316 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:52:28.582900 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:52:28.609041 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:52:28.609750 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

# Add the directory containing the script to sys.path
sys.path.append(os.path.abspath("/home/micasidad/Desktop/yt-comment-analysis/scripts"))

from preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:52:29.333171 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 8ea7242e-060f-4960-a9a2-21889855425c
[0m11:54:21.011386 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'preprocess_data']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/8ea7242e-060f-4960-a9a2-21889855425c?project=sentiment-analysis-410608
gcloud dataproc batches wait '8ea7242e-060f-4960-a9a2-21889855425c' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/c303f181-46fc-4c4a-90ae-852f44d11188/jobs/srvls-batch-c8a22db9-e9ca-48bf-862c-d1c49862e331/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/c303f181-46fc-4c4a-90ae-852f44d11188/jobs/srvls-batch-c8a22db9-e9ca-48bf-862c-d1c49862e331/driveroutput.*
[0m11:54:21.018290 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'preprocess_data']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/8ea7242e-060f-4960-a9a2-21889855425c?project=sentiment-analysis-410608
gcloud dataproc batches wait '8ea7242e-060f-4960-a9a2-21889855425c' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/c303f181-46fc-4c4a-90ae-852f44d11188/jobs/srvls-batch-c8a22db9-e9ca-48bf-862c-d1c49862e331/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/c303f181-46fc-4c4a-90ae-852f44d11188/jobs/srvls-batch-c8a22db9-e9ca-48bf-862c-d1c49862e331/driveroutput.*

[0m11:54:21.022278 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da89f15410>]}
[0m11:54:21.023642 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 112.49s]
[0m11:54:21.024989 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:54:21.025963 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:54:21.027633 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:54:21.028681 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:54:21.029459 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:54:21.035132 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:54:21.036296 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:54:21.040335 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:54:21.772144 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:54:21.773627 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:54:22.178617 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:07247464-0e5d-4f7b-a931-797e97ea932c&page=queryresults
[0m11:54:24.416542 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b680f256-d486-4d66-a074-b22454e89fdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da89f44b10>]}
[0m11:54:24.417268 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.39s]
[0m11:54:24.418100 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:54:24.418628 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:54:24.419198 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:54:24.419779 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:54:24.421078 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:54:24.421455 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:54:24.421898 [info ] [MainThread]: 
[0m11:54:24.422369 [info ] [MainThread]: Finished running 3 table models in 0 hours 1 minutes and 57.55 seconds (117.55s).
[0m11:54:24.423424 [debug] [MainThread]: Command end result
[0m11:54:24.454079 [info ] [MainThread]: 
[0m11:54:24.454503 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:54:24.454821 [info ] [MainThread]: 
[0m11:54:24.455225 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'preprocess_data']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/8ea7242e-060f-4960-a9a2-21889855425c?project=sentiment-analysis-410608
gcloud dataproc batches wait '8ea7242e-060f-4960-a9a2-21889855425c' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/c303f181-46fc-4c4a-90ae-852f44d11188/jobs/srvls-batch-c8a22db9-e9ca-48bf-862c-d1c49862e331/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/c303f181-46fc-4c4a-90ae-852f44d11188/jobs/srvls-batch-c8a22db9-e9ca-48bf-862c-d1c49862e331/driveroutput.*
[0m11:54:24.455572 [info ] [MainThread]: 
[0m11:54:24.455924 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:54:24.456631 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 118.73969, "process_user_time": 3.686079, "process_kernel_time": 0.410579, "process_mem_max_rss": "214840", "process_out_blocks": "2976", "command_success": false, "process_in_blocks": "0"}
[0m11:54:24.457095 [debug] [MainThread]: Command `dbt run` failed at 11:54:24.457012 after 118.74 seconds
[0m11:54:24.457465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72dab2616e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72daaec3a450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72daaec3a890>]}
[0m11:54:24.457816 [debug] [MainThread]: Flushing usage events
[0m11:56:29.567011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70073e80f6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70073e846f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70073e80c150>]}


============================== 11:56:29.569313 | 9661f533-0680-4a42-b1ee-967a08ce7364 ==============================
[0m11:56:29.569313 [info ] [MainThread]: Running with dbt=1.8.5
[0m11:56:29.570215 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:56:30.223796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x700721597690>]}
[0m11:56:30.268679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x700740535510>]}
[0m11:56:30.269284 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m11:56:30.276358 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m11:56:30.354142 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
[0m11:56:30.354594 [debug] [MainThread]: Partial parsing: deleted file: yt_comment_analysis://models/__init__.py
[0m11:56:30.355018 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m11:56:30.612897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x700720166190>]}
[0m11:56:30.677782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70071bc3c150>]}
[0m11:56:30.678351 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m11:56:30.678789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70072845c710>]}
[0m11:56:30.680201 [info ] [MainThread]: 
[0m11:56:30.680855 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:56:30.683824 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m11:56:30.684289 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:56:31.712355 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m11:56:31.714267 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:56:32.336536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70072026a650>]}
[0m11:56:32.339066 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:56:32.339809 [info ] [MainThread]: 
[0m11:56:32.343473 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m11:56:32.344223 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m11:56:32.344951 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m11:56:32.345755 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m11:56:32.388494 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m11:56:32.389171 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m11:56:32.416067 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m11:56:32.416851 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

from macros.preprocess_data import clean_text, get_sentiment_score

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m11:56:33.143112 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: ad508d39-ed7b-4811-ada5-2072d8ea3c21
[0m11:58:28.605237 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'macros']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/ad508d39-ed7b-4811-ada5-2072d8ea3c21?project=sentiment-analysis-410608
gcloud dataproc batches wait 'ad508d39-ed7b-4811-ada5-2072d8ea3c21' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f48fc1af-7649-4e51-87e3-e657cbc4f0b8/jobs/srvls-batch-95641534-a041-4160-944d-81c9e83fee2b/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f48fc1af-7649-4e51-87e3-e657cbc4f0b8/jobs/srvls-batch-95641534-a041-4160-944d-81c9e83fee2b/driveroutput.*
[0m11:58:28.619320 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'macros']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/ad508d39-ed7b-4811-ada5-2072d8ea3c21?project=sentiment-analysis-410608
gcloud dataproc batches wait 'ad508d39-ed7b-4811-ada5-2072d8ea3c21' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f48fc1af-7649-4e51-87e3-e657cbc4f0b8/jobs/srvls-batch-95641534-a041-4160-944d-81c9e83fee2b/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f48fc1af-7649-4e51-87e3-e657cbc4f0b8/jobs/srvls-batch-95641534-a041-4160-944d-81c9e83fee2b/driveroutput.*

[0m11:58:28.622278 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x700720125310>]}
[0m11:58:28.623387 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 116.28s]
[0m11:58:28.624647 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m11:58:28.625670 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m11:58:28.627611 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m11:58:28.628540 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m11:58:28.629591 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m11:58:28.635154 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m11:58:28.636141 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m11:58:28.638398 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:58:29.486300 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m11:58:29.487119 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m11:58:30.902730 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:ecbb374b-9196-4931-a7f3-6c5b876883a8&page=queryresults
[0m11:58:33.296443 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9661f533-0680-4a42-b1ee-967a08ce7364', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x700719b1b190>]}
[0m11:58:33.297218 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.67s]
[0m11:58:33.298000 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m11:58:33.298515 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m11:58:33.298999 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m11:58:33.299527 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m11:58:33.300844 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:58:33.301222 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m11:58:33.301654 [info ] [MainThread]: 
[0m11:58:33.302114 [info ] [MainThread]: Finished running 3 table models in 0 hours 2 minutes and 2.62 seconds (122.62s).
[0m11:58:33.303133 [debug] [MainThread]: Command end result
[0m11:58:33.338759 [info ] [MainThread]: 
[0m11:58:33.339311 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:58:33.339802 [info ] [MainThread]: 
[0m11:58:33.340408 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'macros']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/ad508d39-ed7b-4811-ada5-2072d8ea3c21?project=sentiment-analysis-410608
gcloud dataproc batches wait 'ad508d39-ed7b-4811-ada5-2072d8ea3c21' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f48fc1af-7649-4e51-87e3-e657cbc4f0b8/jobs/srvls-batch-95641534-a041-4160-944d-81c9e83fee2b/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/f48fc1af-7649-4e51-87e3-e657cbc4f0b8/jobs/srvls-batch-95641534-a041-4160-944d-81c9e83fee2b/driveroutput.*
[0m11:58:33.340932 [info ] [MainThread]: 
[0m11:58:33.341455 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m11:58:33.342575 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 123.812874, "process_user_time": 3.727905, "process_kernel_time": 0.381822, "process_mem_max_rss": "214752", "process_in_blocks": "16", "process_out_blocks": "2952", "command_success": false}
[0m11:58:33.343078 [debug] [MainThread]: Command `dbt run` failed at 11:58:33.342989 after 123.81 seconds
[0m11:58:33.343473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70073e80e710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7007421f6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70073e805110>]}
[0m11:58:33.343846 [debug] [MainThread]: Flushing usage events
[0m12:00:26.143191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4c8424050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4c845ea50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4c8427390>]}


============================== 12:00:26.145231 | b2864bfe-2682-45ce-912e-d7578655e98d ==============================
[0m12:00:26.145231 [info ] [MainThread]: Running with dbt=1.8.5
[0m12:00:26.145905 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:00:26.780621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4a5e6b910>]}
[0m12:00:26.818434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4a5b81a90>]}
[0m12:00:26.818994 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m12:00:26.825473 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m12:00:26.931005 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:00:26.931515 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m12:00:26.931893 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/videos.py
[0m12:00:27.176332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4c844d850>]}
[0m12:00:27.235055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4a5895910>]}
[0m12:00:27.235501 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m12:00:27.235895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4c8424f10>]}
[0m12:00:27.237243 [info ] [MainThread]: 
[0m12:00:27.237871 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:00:27.241181 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m12:00:27.241787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:00:28.871251 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m12:00:28.873194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:00:29.592638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4a5c41810>]}
[0m12:00:29.594996 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:00:29.595777 [info ] [MainThread]: 
[0m12:00:29.598773 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m12:00:29.599471 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m12:00:29.600139 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m12:00:29.600736 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m12:00:29.628230 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m12:00:29.628871 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m12:00:29.656078 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m12:00:29.656977 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

import re
from html import unescape
import string
from textblob import TextBlob

'''
TODO:
- Transform DURATION from STRING to INT64
- Transform PUBLISHED_AT from STRING to TIMESTAMP
- Clean TEXT of raw_comments
- Estimate SCORE of comments
- Calculate AVERAGE_SCORE, MAX_SCORE, MIN_SCORE of videos
- Create users table
'''

def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Transform DURATION
def iso8601_to_minutes(duration: str) -> int:
    # Use regex to find hours, minutes, and seconds in the duration string
    hours = re.search(r'(\d+)H', duration)
    minutes = re.search(r'(\d+)M', duration)
    seconds = re.search(r'(\d+)S', duration)
    
    # Convert found values to integers, default to 0 if not found
    hours = int(hours.group(1)) if hours else 0
    minutes = int(minutes.group(1)) if minutes else 0
    seconds = int(seconds.group(1)) if seconds else 0
    
    # Calculate total minutes
    total_minutes = hours * 60 + minutes + (seconds / 60)
    
    return int(total_minutes)

def get_sentiment_score(text):
    # Create a TextBlob object
    blob = TextBlob(text)

    # Return the sentiment score
    return blob.sentiment.polarity

def model(dbt, session):
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m12:00:30.310476 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: eb0f8776-b37b-4708-82ec-bd90007641a5
[0m12:02:30.197406 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/eb0f8776-b37b-4708-82ec-bd90007641a5?project=sentiment-analysis-410608
gcloud dataproc batches wait 'eb0f8776-b37b-4708-82ec-bd90007641a5' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/1a02d4eb-24b8-4664-99e9-637cc86fa53b/jobs/srvls-batch-7c4bf865-298a-4c75-90ea-dac40e0ef151/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/1a02d4eb-24b8-4664-99e9-637cc86fa53b/jobs/srvls-batch-7c4bf865-298a-4c75-90ea-dac40e0ef151/driveroutput.*
[0m12:02:30.204639 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/eb0f8776-b37b-4708-82ec-bd90007641a5?project=sentiment-analysis-410608
gcloud dataproc batches wait 'eb0f8776-b37b-4708-82ec-bd90007641a5' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/1a02d4eb-24b8-4664-99e9-637cc86fa53b/jobs/srvls-batch-7c4bf865-298a-4c75-90ea-dac40e0ef151/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/1a02d4eb-24b8-4664-99e9-637cc86fa53b/jobs/srvls-batch-7c4bf865-298a-4c75-90ea-dac40e0ef151/driveroutput.*

[0m12:02:30.208554 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef49773a190>]}
[0m12:02:30.209511 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 120.61s]
[0m12:02:30.210526 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m12:02:30.211178 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m12:02:30.212614 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m12:02:30.213825 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m12:02:30.214454 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m12:02:30.220193 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m12:02:30.221046 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m12:02:30.224042 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:02:30.931225 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m12:02:30.932675 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m12:02:32.259969 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:8e0ad0e0-5d95-46c9-b1b8-76551cb38645&page=queryresults
[0m12:02:34.367619 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2864bfe-2682-45ce-912e-d7578655e98d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4a58a6650>]}
[0m12:02:34.368569 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.15s]
[0m12:02:34.369473 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m12:02:34.370132 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m12:02:34.370839 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m12:02:34.371536 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m12:02:34.373027 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:02:34.373453 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m12:02:34.373956 [info ] [MainThread]: 
[0m12:02:34.374419 [info ] [MainThread]: Finished running 3 table models in 0 hours 2 minutes and 7.14 seconds (127.14s).
[0m12:02:34.375613 [debug] [MainThread]: Command end result
[0m12:02:34.404642 [info ] [MainThread]: 
[0m12:02:34.405199 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:02:34.405574 [info ] [MainThread]: 
[0m12:02:34.406124 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/eb0f8776-b37b-4708-82ec-bd90007641a5?project=sentiment-analysis-410608
gcloud dataproc batches wait 'eb0f8776-b37b-4708-82ec-bd90007641a5' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/1a02d4eb-24b8-4664-99e9-637cc86fa53b/jobs/srvls-batch-7c4bf865-298a-4c75-90ea-dac40e0ef151/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/1a02d4eb-24b8-4664-99e9-637cc86fa53b/jobs/srvls-batch-7c4bf865-298a-4c75-90ea-dac40e0ef151/driveroutput.*
[0m12:02:34.406553 [info ] [MainThread]: 
[0m12:02:34.406992 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m12:02:34.407889 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 128.30084, "process_user_time": 3.651979, "process_kernel_time": 0.417369, "process_mem_max_rss": "215080", "process_out_blocks": "2976", "command_success": false, "process_in_blocks": "0"}
[0m12:02:34.408428 [debug] [MainThread]: Command `dbt run` failed at 12:02:34.408340 after 128.30 seconds
[0m12:02:34.408807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4cbe16e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4cbeb9c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef4c845ef90>]}
[0m12:02:34.409167 [debug] [MainThread]: Flushing usage events
[0m12:11:46.401320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7180853abe10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7180853deb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7180853de910>]}


============================== 12:11:46.403431 | 541e281b-202d-4663-8920-8d5847db72d7 ==============================
[0m12:11:46.403431 [info ] [MainThread]: Running with dbt=1.8.5
[0m12:11:46.404197 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:11:47.050030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718063416b90>]}
[0m12:11:47.088039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71808710d5d0>]}
[0m12:11:47.088595 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m12:11:47.095040 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m12:11:47.170468 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:11:47.170973 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/videos.py
[0m12:11:47.171345 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m12:11:47.419161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7180629dfbd0>]}
[0m12:11:47.476235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718062a36450>]}
[0m12:11:47.476709 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m12:11:47.477112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718062cdd190>]}
[0m12:11:47.478529 [info ] [MainThread]: 
[0m12:11:47.479108 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:11:47.481988 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m12:11:47.482498 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:11:48.559591 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m12:11:48.560445 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:11:49.191050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71806310a950>]}
[0m12:11:49.192225 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:11:49.193597 [info ] [MainThread]: 
[0m12:11:49.198597 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m12:11:49.200726 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m12:11:49.202520 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m12:11:49.203991 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m12:11:49.257079 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m12:11:49.257776 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m12:11:49.284661 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m12:11:49.285461 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

import re
from html import unescape
import string
from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # Create a TextBlob object
    blob = TextBlob(text)

    # Return the sentiment score
    return blob.sentiment.polarity

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["textblob", "re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m12:11:49.937167 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 53d8b060-3b97-4c80-8081-7cfe74f658a2
[0m12:13:40.132412 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/53d8b060-3b97-4c80-8081-7cfe74f658a2?project=sentiment-analysis-410608
gcloud dataproc batches wait '53d8b060-3b97-4c80-8081-7cfe74f658a2' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ff1ed9f0-badc-48f0-b41a-2703c118d14a/jobs/srvls-batch-48e85b0e-e613-468f-b7e8-fe87a69b967a/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ff1ed9f0-badc-48f0-b41a-2703c118d14a/jobs/srvls-batch-48e85b0e-e613-468f-b7e8-fe87a69b967a/driveroutput.*
[0m12:13:40.139011 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/53d8b060-3b97-4c80-8081-7cfe74f658a2?project=sentiment-analysis-410608
gcloud dataproc batches wait '53d8b060-3b97-4c80-8081-7cfe74f658a2' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ff1ed9f0-badc-48f0-b41a-2703c118d14a/jobs/srvls-batch-48e85b0e-e613-468f-b7e8-fe87a69b967a/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ff1ed9f0-badc-48f0-b41a-2703c118d14a/jobs/srvls-batch-48e85b0e-e613-468f-b7e8-fe87a69b967a/driveroutput.*

[0m12:13:40.142136 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718063436d90>]}
[0m12:13:40.143082 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 110.94s]
[0m12:13:40.143918 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m12:13:40.144524 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m12:13:40.145888 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m12:13:40.146698 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m12:13:40.147251 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m12:13:40.152104 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m12:13:40.152885 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m12:13:40.155969 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:13:40.818526 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m12:13:40.819377 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m12:13:42.410423 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:16764bac-389e-4013-b4e6-e1ce251e0e41&page=queryresults
[0m12:13:44.440970 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '541e281b-202d-4663-8920-8d5847db72d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71806291b690>]}
[0m12:13:44.441655 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.29s]
[0m12:13:44.442322 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m12:13:44.442810 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m12:13:44.443378 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m12:13:44.443876 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m12:13:44.445049 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:13:44.445410 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m12:13:44.445827 [info ] [MainThread]: 
[0m12:13:44.446234 [info ] [MainThread]: Finished running 3 table models in 0 hours 1 minutes and 56.97 seconds (116.97s).
[0m12:13:44.447098 [debug] [MainThread]: Command end result
[0m12:13:44.472445 [info ] [MainThread]: 
[0m12:13:44.473024 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:13:44.473649 [info ] [MainThread]: 
[0m12:13:44.474364 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/53d8b060-3b97-4c80-8081-7cfe74f658a2?project=sentiment-analysis-410608
gcloud dataproc batches wait '53d8b060-3b97-4c80-8081-7cfe74f658a2' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ff1ed9f0-badc-48f0-b41a-2703c118d14a/jobs/srvls-batch-48e85b0e-e613-468f-b7e8-fe87a69b967a/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ff1ed9f0-badc-48f0-b41a-2703c118d14a/jobs/srvls-batch-48e85b0e-e613-468f-b7e8-fe87a69b967a/driveroutput.*
[0m12:13:44.474862 [info ] [MainThread]: 
[0m12:13:44.475202 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m12:13:44.475981 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 118.111465, "process_user_time": 3.64253, "process_kernel_time": 0.359805, "process_mem_max_rss": "215048", "process_out_blocks": "2960", "command_success": false, "process_in_blocks": "0"}
[0m12:13:44.476443 [debug] [MainThread]: Command `dbt run` failed at 12:13:44.476361 after 118.11 seconds
[0m12:13:44.476808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718088df6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7180853aa490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718088d1d3d0>]}
[0m12:13:44.477193 [debug] [MainThread]: Flushing usage events
[0m12:20:42.855298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d5383b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d53b7210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d53b73d0>]}


============================== 12:20:42.857316 | fab8f431-6531-4aa7-9ae0-5d56871f8c75 ==============================
[0m12:20:42.857316 [info ] [MainThread]: Running with dbt=1.8.5
[0m12:20:42.857989 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:20:43.599037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d53d1a10>]}
[0m12:20:43.642388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d70e8dd0>]}
[0m12:20:43.642963 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m12:20:43.650742 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m12:20:43.735494 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m12:20:43.736067 [debug] [MainThread]: Partial parsing: added file: yt_comment_analysis://models/config.yml
[0m12:20:43.958608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3b2900450>]}
[0m12:20:44.018722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3b29ba990>]}
[0m12:20:44.019174 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m12:20:44.019564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3b29a6fd0>]}
[0m12:20:44.020866 [info ] [MainThread]: 
[0m12:20:44.021390 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:20:44.024333 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m12:20:44.024710 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:20:45.291037 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m12:20:45.291845 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:20:46.012415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3b2d82810>]}
[0m12:20:46.015615 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:20:46.017757 [info ] [MainThread]: 
[0m12:20:46.028377 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m12:20:46.032616 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m12:20:46.035185 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m12:20:46.036438 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m12:20:46.076452 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m12:20:46.077185 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m12:20:46.111300 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m12:20:46.112182 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

import re
from html import unescape
import string
from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # Create a TextBlob object
    blob = TextBlob(text)

    # Return the sentiment score
    return blob.sentiment.polarity

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["textblob", "re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m12:20:46.840554 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 9e2b3fb9-d37e-4f1b-8263-9dd3a0114c84
[0m12:22:56.825621 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/9e2b3fb9-d37e-4f1b-8263-9dd3a0114c84?project=sentiment-analysis-410608
gcloud dataproc batches wait '9e2b3fb9-d37e-4f1b-8263-9dd3a0114c84' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ead17ecb-5c2c-4d95-af04-6ad2f3425174/jobs/srvls-batch-bc3a1045-a828-4aaf-b91b-cf314d3224e6/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ead17ecb-5c2c-4d95-af04-6ad2f3425174/jobs/srvls-batch-bc3a1045-a828-4aaf-b91b-cf314d3224e6/driveroutput.*
[0m12:22:56.832504 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/9e2b3fb9-d37e-4f1b-8263-9dd3a0114c84?project=sentiment-analysis-410608
gcloud dataproc batches wait '9e2b3fb9-d37e-4f1b-8263-9dd3a0114c84' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ead17ecb-5c2c-4d95-af04-6ad2f3425174/jobs/srvls-batch-bc3a1045-a828-4aaf-b91b-cf314d3224e6/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ead17ecb-5c2c-4d95-af04-6ad2f3425174/jobs/srvls-batch-bc3a1045-a828-4aaf-b91b-cf314d3224e6/driveroutput.*

[0m12:22:56.835518 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3b2843950>]}
[0m12:22:56.836470 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 130.80s]
[0m12:22:56.837359 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m12:22:56.838027 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m12:22:56.839458 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m12:22:56.840209 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m12:22:56.840757 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m12:22:56.844864 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m12:22:56.846115 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m12:22:56.849361 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:22:57.513186 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m12:22:57.514607 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m12:22:59.029106 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:1bd70178-240a-43e3-88e3-6b114784348b&page=queryresults
[0m12:23:01.427409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fab8f431-6531-4aa7-9ae0-5d56871f8c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3b0711950>]}
[0m12:23:01.428517 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.59s]
[0m12:23:01.429505 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m12:23:01.430178 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m12:23:01.430906 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m12:23:01.431596 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m12:23:01.433148 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:23:01.433663 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m12:23:01.434276 [info ] [MainThread]: 
[0m12:23:01.434779 [info ] [MainThread]: Finished running 3 table models in 0 hours 2 minutes and 17.41 seconds (137.41s).
[0m12:23:01.436051 [debug] [MainThread]: Command end result
[0m12:23:01.465575 [info ] [MainThread]: 
[0m12:23:01.466126 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:23:01.466553 [info ] [MainThread]: 
[0m12:23:01.467026 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/9e2b3fb9-d37e-4f1b-8263-9dd3a0114c84?project=sentiment-analysis-410608
gcloud dataproc batches wait '9e2b3fb9-d37e-4f1b-8263-9dd3a0114c84' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ead17ecb-5c2c-4d95-af04-6ad2f3425174/jobs/srvls-batch-bc3a1045-a828-4aaf-b91b-cf314d3224e6/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/ead17ecb-5c2c-4d95-af04-6ad2f3425174/jobs/srvls-batch-bc3a1045-a828-4aaf-b91b-cf314d3224e6/driveroutput.*
[0m12:23:01.467427 [info ] [MainThread]: 
[0m12:23:01.467899 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m12:23:01.468805 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 138.64969, "process_user_time": 3.795263, "process_kernel_time": 0.382715, "process_mem_max_rss": "214132", "process_out_blocks": "2960", "command_success": false, "process_in_blocks": "0"}
[0m12:23:01.469437 [debug] [MainThread]: Command `dbt run` failed at 12:23:01.469343 after 138.65 seconds
[0m12:23:01.469994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d8db6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d8e59c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d3d8e59910>]}
[0m12:23:01.470465 [debug] [MainThread]: Flushing usage events
[0m16:15:46.389595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726476db7ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726476db4610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726476db7fd0>]}


============================== 16:15:46.391678 | f01dc7b7-116e-4335-899b-476e440809f5 ==============================
[0m16:15:46.391678 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:15:46.392491 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:15:47.035041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726458632ad0>]}
[0m16:15:47.072314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726458509a90>]}
[0m16:15:47.072825 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:15:47.079637 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:15:47.183809 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:15:47.184177 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:15:47.283373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726458a6f910>]}
[0m16:15:47.352432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72645842e150>]}
[0m16:15:47.352972 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:15:47.353396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7264585f6190>]}
[0m16:15:47.355193 [info ] [MainThread]: 
[0m16:15:47.356000 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:15:47.360135 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:15:47.360755 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:15:48.359633 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:15:48.361529 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:15:48.971897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7264588549d0>]}
[0m16:15:48.974514 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:15:48.976448 [info ] [MainThread]: 
[0m16:15:48.983881 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:15:48.986101 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m16:15:48.988128 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:15:48.989919 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:15:49.036384 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:15:49.037008 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:15:49.064475 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m16:15:49.065196 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import pandas as pd

import re
from html import unescape
import string
from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # Create a TextBlob object
    blob = TextBlob(text)

    # Return the sentiment score
    return blob.sentiment.polarity

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["textblob", "re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m16:15:49.688876 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 51abf0b7-0757-4245-a938-839d8803d84e
[0m16:17:54.091847 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/51abf0b7-0757-4245-a938-839d8803d84e?project=sentiment-analysis-410608
gcloud dataproc batches wait '51abf0b7-0757-4245-a938-839d8803d84e' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/447b9a4d-28d8-452a-8412-933d83f26b44/jobs/srvls-batch-339b0dc3-aada-43b8-8367-93e7b1627962/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/447b9a4d-28d8-452a-8412-933d83f26b44/jobs/srvls-batch-339b0dc3-aada-43b8-8367-93e7b1627962/driveroutput.*
[0m16:17:54.107392 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/51abf0b7-0757-4245-a938-839d8803d84e?project=sentiment-analysis-410608
gcloud dataproc batches wait '51abf0b7-0757-4245-a938-839d8803d84e' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/447b9a4d-28d8-452a-8412-933d83f26b44/jobs/srvls-batch-339b0dc3-aada-43b8-8367-93e7b1627962/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/447b9a4d-28d8-452a-8412-933d83f26b44/jobs/srvls-batch-339b0dc3-aada-43b8-8367-93e7b1627962/driveroutput.*

[0m16:17:54.116828 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726458670050>]}
[0m16:17:54.120177 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 125.13s]
[0m16:17:54.123378 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:17:54.126586 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:17:54.132731 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:17:54.135119 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:17:54.136919 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:17:54.145379 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:17:54.147272 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:17:54.155669 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:17:54.802011 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:17:54.802724 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:17:55.443713 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:9b59077b-45a1-4065-8255-fa2b144fc062&page=queryresults
[0m16:17:57.618450 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f01dc7b7-116e-4335-899b-476e440809f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7264586315d0>]}
[0m16:17:57.619128 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.48s]
[0m16:17:57.619819 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:17:57.620298 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:17:57.620741 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:17:57.621388 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:17:57.622619 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:17:57.622962 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:17:57.623360 [info ] [MainThread]: 
[0m16:17:57.623726 [info ] [MainThread]: Finished running 3 table models in 0 hours 2 minutes and 10.27 seconds (130.27s).
[0m16:17:57.624613 [debug] [MainThread]: Command end result
[0m16:17:57.646386 [info ] [MainThread]: 
[0m16:17:57.646804 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:17:57.647116 [info ] [MainThread]: 
[0m16:17:57.647505 [error] [MainThread]:   Job failed with message [ModuleNotFoundError: No module named 'textblob']. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/51abf0b7-0757-4245-a938-839d8803d84e?project=sentiment-analysis-410608
gcloud dataproc batches wait '51abf0b7-0757-4245-a938-839d8803d84e' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/447b9a4d-28d8-452a-8412-933d83f26b44/jobs/srvls-batch-339b0dc3-aada-43b8-8367-93e7b1627962/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/447b9a4d-28d8-452a-8412-933d83f26b44/jobs/srvls-batch-339b0dc3-aada-43b8-8367-93e7b1627962/driveroutput.*
[0m16:17:57.647842 [info ] [MainThread]: 
[0m16:17:57.648165 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m16:17:57.648845 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 131.29604, "process_user_time": 3.541915, "process_kernel_time": 0.423577, "process_mem_max_rss": "212460", "process_out_blocks": "2000", "command_success": false, "process_in_blocks": "0"}
[0m16:17:57.649300 [debug] [MainThread]: Command `dbt run` failed at 16:17:57.649218 after 131.30 seconds
[0m16:17:57.649672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72647a7d6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726458deccd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72645856bf50>]}
[0m16:17:57.650021 [debug] [MainThread]: Flushing usage events
[0m16:27:38.183057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79663a7bf610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79663a7bfc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79663a7bf850>]}


============================== 16:27:38.185119 | fd4988be-69a8-4d78-bf51-0661d618123e ==============================
[0m16:27:38.185119 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:27:38.185811 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m16:27:38.851067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79661c1fee50>]}
[0m16:27:38.887942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79663c501290>]}
[0m16:27:38.888471 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:27:38.895590 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:27:38.971853 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:27:38.972367 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m16:27:39.254152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79661c05fad0>]}
[0m16:27:39.311474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79661c05e0d0>]}
[0m16:27:39.311939 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:27:39.312312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796617d6d610>]}
[0m16:27:39.313658 [info ] [MainThread]: 
[0m16:27:39.314294 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:27:39.317153 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:27:39.317618 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:27:40.361420 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:27:40.363602 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:27:40.996603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79661c0a5750>]}
[0m16:27:40.997581 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:27:40.998384 [info ] [MainThread]: 
[0m16:27:41.001554 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:27:41.002606 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m16:27:41.003539 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:27:41.004362 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:27:41.028020 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:27:41.028686 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:27:41.055636 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m16:27:41.056446 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import subprocess
import pandas as pd

import re
from html import unescape
import string

# Install textblob in the Dataproc environment
subprocess.check_call([sys.executable, "-m", "pip", "install", "textblob"])

from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # Create a TextBlob object
    blob = TextBlob(text)

    # Return the sentiment score
    return blob.sentiment.polarity

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["textblob", "re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m16:27:41.690868 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 25f8d0f8-fa88-4a45-a360-8f7bae8b24da
[0m16:32:43.306457 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Operation did not complete within the designated timeout of 300 seconds.
[0m16:32:43.323482 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 47, in poll_batch_job
    raise ValueError(
ValueError: Operation did not complete within the designated timeout of 300 seconds.

[0m16:32:43.331926 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796617f600d0>]}
[0m16:32:43.333802 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 302.33s]
[0m16:32:43.335408 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:32:43.336480 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:32:43.339681 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:32:43.341008 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:32:43.342492 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:32:43.348953 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:32:43.350592 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:32:43.355621 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:32:44.156898 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:32:44.158538 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:32:44.843000 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:ce1a9de5-440e-4151-890f-e68cc88de5d9&page=queryresults
[0m16:32:46.938249 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd4988be-69a8-4d78-bf51-0661d618123e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796617b4efd0>]}
[0m16:32:46.939153 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.60s]
[0m16:32:46.939911 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:32:46.940426 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:32:46.941033 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:32:46.941560 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:32:46.942886 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:32:46.943599 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:32:46.944101 [info ] [MainThread]: 
[0m16:32:46.944570 [info ] [MainThread]: Finished running 3 table models in 0 hours 5 minutes and 7.63 seconds (307.63s).
[0m16:32:46.945510 [debug] [MainThread]: Command end result
[0m16:32:46.975310 [info ] [MainThread]: 
[0m16:32:46.975831 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:32:46.976162 [info ] [MainThread]: 
[0m16:32:46.976547 [error] [MainThread]:   Operation did not complete within the designated timeout of 300 seconds.
[0m16:32:46.976916 [info ] [MainThread]: 
[0m16:32:46.977369 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m16:32:46.978342 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 308.83185, "process_user_time": 4.292763, "process_kernel_time": 0.70931, "process_mem_max_rss": "215876", "process_out_blocks": "2984", "command_success": false, "process_in_blocks": "0"}
[0m16:32:46.978839 [debug] [MainThread]: Command `dbt run` failed at 16:32:46.978755 after 308.83 seconds
[0m16:32:46.979212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79663e1d6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79663a7e0dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79663e279910>]}
[0m16:32:46.979562 [debug] [MainThread]: Flushing usage events
[0m16:37:35.250957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66db51f3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66db556810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66db51f910>]}


============================== 16:37:35.253098 | eec4a628-7533-4ecf-9dbb-90847e5980b7 ==============================
[0m16:37:35.253098 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:37:35.253663 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:37:35.898210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66db524650>]}
[0m16:37:35.935502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66dd261410>]}
[0m16:37:35.936029 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:37:35.942534 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:37:36.021314 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:37:36.021831 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m16:37:36.316907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66bc97e750>]}
[0m16:37:36.371956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66bce3ded0>]}
[0m16:37:36.372400 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:37:36.372804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66bca42f50>]}
[0m16:37:36.374214 [info ] [MainThread]: 
[0m16:37:36.374796 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:37:36.377615 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:37:36.378131 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:37:37.395882 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:37:37.398072 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:37:38.113849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66bce6b7d0>]}
[0m16:37:38.116437 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:37:38.118323 [info ] [MainThread]: 
[0m16:37:38.127188 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:37:38.130447 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m16:37:38.132857 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:37:38.134760 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:37:38.186228 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:37:38.186896 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:37:38.218217 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m16:37:38.218985 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import subprocess
import pandas as pd

import re
from html import unescape
import string

# from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # # Create a TextBlob object
    # blob = TextBlob(text)

    # # Return the sentiment score
    # return blob.sentiment.polarity
    return 0

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').to_pandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m16:37:38.853785 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: fe28f880-889d-42c7-b765-e6574affa4de
[0m16:39:34.679659 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [AttributeError: 'DataFrame' object has no attribute 'to_pandas'. Did you mean: 'toPandas'?]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/fe28f880-889d-42c7-b765-e6574affa4de?project=sentiment-analysis-410608
gcloud dataproc batches wait 'fe28f880-889d-42c7-b765-e6574affa4de' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/dd0fac67-414e-4523-9791-87d612c0027a/jobs/srvls-batch-b96ff2fe-5ffa-4dd4-a021-09253094a5dc/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/dd0fac67-414e-4523-9791-87d612c0027a/jobs/srvls-batch-b96ff2fe-5ffa-4dd4-a021-09253094a5dc/driveroutput.*
[0m16:39:34.695360 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [AttributeError: 'DataFrame' object has no attribute 'to_pandas'. Did you mean: 'toPandas'?]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/fe28f880-889d-42c7-b765-e6574affa4de?project=sentiment-analysis-410608
gcloud dataproc batches wait 'fe28f880-889d-42c7-b765-e6574affa4de' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/dd0fac67-414e-4523-9791-87d612c0027a/jobs/srvls-batch-b96ff2fe-5ffa-4dd4-a021-09253094a5dc/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/dd0fac67-414e-4523-9791-87d612c0027a/jobs/srvls-batch-b96ff2fe-5ffa-4dd4-a021-09253094a5dc/driveroutput.*

[0m16:39:34.705299 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66bc92e750>]}
[0m16:39:34.708376 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 116.57s]
[0m16:39:34.711378 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:39:34.713529 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:39:34.719987 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:39:34.722316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:39:34.723312 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:39:34.729251 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:39:34.730597 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:39:34.734485 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:39:35.584723 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:39:35.586017 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:39:37.205736 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:3cbc3710-ef4c-4890-9cfb-6b3fc10b995d&page=queryresults
[0m16:39:39.349908 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eec4a628-7533-4ecf-9dbb-90847e5980b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66bc90f110>]}
[0m16:39:39.350546 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 4.63s]
[0m16:39:39.351188 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:39:39.351620 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:39:39.352110 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:39:39.352553 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:39:39.353818 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:39:39.354184 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:39:39.354580 [info ] [MainThread]: 
[0m16:39:39.354945 [info ] [MainThread]: Finished running 3 table models in 0 hours 2 minutes and 2.98 seconds (122.98s).
[0m16:39:39.355908 [debug] [MainThread]: Command end result
[0m16:39:39.402655 [info ] [MainThread]: 
[0m16:39:39.404038 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:39:39.405504 [info ] [MainThread]: 
[0m16:39:39.407058 [error] [MainThread]:   Job failed with message [AttributeError: 'DataFrame' object has no attribute 'to_pandas'. Did you mean: 'toPandas'?]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/fe28f880-889d-42c7-b765-e6574affa4de?project=sentiment-analysis-410608
gcloud dataproc batches wait 'fe28f880-889d-42c7-b765-e6574affa4de' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/dd0fac67-414e-4523-9791-87d612c0027a/jobs/srvls-batch-b96ff2fe-5ffa-4dd4-a021-09253094a5dc/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/dd0fac67-414e-4523-9791-87d612c0027a/jobs/srvls-batch-b96ff2fe-5ffa-4dd4-a021-09253094a5dc/driveroutput.*
[0m16:39:39.408519 [info ] [MainThread]: 
[0m16:39:39.409683 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m16:39:39.411608 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 124.20871, "process_user_time": 4.058141, "process_kernel_time": 0.322854, "process_mem_max_rss": "216068", "process_out_blocks": "2968", "command_success": false, "process_in_blocks": "0"}
[0m16:39:39.412990 [debug] [MainThread]: Command `dbt run` failed at 16:39:39.412797 after 124.21 seconds
[0m16:39:39.414144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66def1ae50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66defbdc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a66db516cd0>]}
[0m16:39:39.415262 [debug] [MainThread]: Flushing usage events
[0m16:42:00.598778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757aee1abb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757aee1de8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757aee1de910>]}


============================== 16:42:00.600885 | 10aa1b55-12cd-4c44-bc1e-35c8244bd708 ==============================
[0m16:42:00.600885 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:42:00.601622 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:42:01.252473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757aee214110>]}
[0m16:42:01.290108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757aeff0d350>]}
[0m16:42:01.290631 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:42:01.297146 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:42:01.375229 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m16:42:01.375775 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/videos.py
[0m16:42:01.376197 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m16:42:01.670540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757acb7ef210>]}
[0m16:42:01.728229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757acb6d3510>]}
[0m16:42:01.728702 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:42:01.729069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757acb7599d0>]}
[0m16:42:01.730430 [info ] [MainThread]: 
[0m16:42:01.730947 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:42:01.733806 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:42:01.734225 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:02.609803 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:42:02.612132 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:42:03.288838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757acb7ed690>]}
[0m16:42:03.290011 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:42:03.290922 [info ] [MainThread]: 
[0m16:42:03.293934 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:42:03.295041 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m16:42:03.295987 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:42:03.296663 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:42:03.329646 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:42:03.330443 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:42:03.361471 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m16:42:03.362200 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import subprocess
import pandas as pd

import re
from html import unescape
import string

# from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # # Create a TextBlob object
    # blob = TextBlob(text)

    # # Return the sentiment score
    # return blob.sentiment.polarity
    return 0

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments')

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m16:42:03.991819 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 5cbccf16-f576-45c8-9954-e20b0094f358
[0m16:43:58.668994 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/comments.py[0m
Job failed with message [TypeError: 'Column' object is not callable]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/5cbccf16-f576-45c8-9954-e20b0094f358?project=sentiment-analysis-410608
gcloud dataproc batches wait '5cbccf16-f576-45c8-9954-e20b0094f358' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/a2e195c3-5d3b-4fa3-aebe-ba5f247b3106/jobs/srvls-batch-4b088123-8b7e-4f4c-a5dd-6834bf6aee7b/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/a2e195c3-5d3b-4fa3-aebe-ba5f247b3106/jobs/srvls-batch-4b088123-8b7e-4f4c-a5dd-6834bf6aee7b/driveroutput.*
[0m16:43:58.686218 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [TypeError: 'Column' object is not callable]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/5cbccf16-f576-45c8-9954-e20b0094f358?project=sentiment-analysis-410608
gcloud dataproc batches wait '5cbccf16-f576-45c8-9954-e20b0094f358' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/a2e195c3-5d3b-4fa3-aebe-ba5f247b3106/jobs/srvls-batch-4b088123-8b7e-4f4c-a5dd-6834bf6aee7b/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/a2e195c3-5d3b-4fa3-aebe-ba5f247b3106/jobs/srvls-batch-4b088123-8b7e-4f4c-a5dd-6834bf6aee7b/driveroutput.*

[0m16:43:58.696789 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757acb5854d0>]}
[0m16:43:58.700065 [error] [Thread-1 (]: 1 of 3 ERROR creating python table model youtube_sentiment.comments ............ [[31mERROR[0m in 115.40s]
[0m16:43:58.703984 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:43:58.706676 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:43:58.711263 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:43:58.713407 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:43:58.714933 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:43:58.720873 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:43:58.722631 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:43:58.727727 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:43:59.376350 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:43:59.377919 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:43:59.753855 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:89b8ad12-04ff-45c7-8eb0-4a75c6a7e859&page=queryresults
[0m16:44:01.943956 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10aa1b55-12cd-4c44-bc1e-35c8244bd708', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757acb961790>]}
[0m16:44:01.944736 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.23s]
[0m16:44:01.945457 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:44:01.946003 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:44:01.946545 [info ] [Thread-1 (]: 3 of 3 SKIP relation youtube_sentiment.videos .................................. [[33mSKIP[0m]
[0m16:44:01.947081 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:44:01.948352 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:44:01.948733 [debug] [MainThread]: Connection 'model.yt_comment_analysis.users' was properly closed.
[0m16:44:01.949162 [info ] [MainThread]: 
[0m16:44:01.949571 [info ] [MainThread]: Finished running 3 table models in 0 hours 2 minutes and 0.22 seconds (120.22s).
[0m16:44:01.950665 [debug] [MainThread]: Command end result
[0m16:44:01.978122 [info ] [MainThread]: 
[0m16:44:01.978655 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:44:01.979039 [info ] [MainThread]: 
[0m16:44:01.979470 [error] [MainThread]:   Job failed with message [TypeError: 'Column' object is not callable]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/5cbccf16-f576-45c8-9954-e20b0094f358?project=sentiment-analysis-410608
gcloud dataproc batches wait '5cbccf16-f576-45c8-9954-e20b0094f358' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/a2e195c3-5d3b-4fa3-aebe-ba5f247b3106/jobs/srvls-batch-4b088123-8b7e-4f4c-a5dd-6834bf6aee7b/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/a2e195c3-5d3b-4fa3-aebe-ba5f247b3106/jobs/srvls-batch-4b088123-8b7e-4f4c-a5dd-6834bf6aee7b/driveroutput.*
[0m16:44:01.979852 [info ] [MainThread]: 
[0m16:44:01.980373 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m16:44:01.981440 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 121.41941, "process_user_time": 3.745544, "process_kernel_time": 0.421976, "process_mem_max_rss": "215716", "process_out_blocks": "2968", "command_success": false, "process_in_blocks": "0"}
[0m16:44:01.982152 [debug] [MainThread]: Command `dbt run` failed at 16:44:01.982057 after 121.42 seconds
[0m16:44:01.982547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757af1bf6e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757aee1aa6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x757aee1aa010>]}
[0m16:44:01.982904 [debug] [MainThread]: Flushing usage events
[0m16:46:42.002528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ae92da050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ae930e8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ae930ea50>]}


============================== 16:46:42.004720 | 89250fee-dab1-463b-8c9d-4f162acadd7f ==============================
[0m16:46:42.004720 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:46:42.005337 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:46:42.716775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac6bdf4d0>]}
[0m16:46:42.760417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac702aa90>]}
[0m16:46:42.761244 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:46:42.768267 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:46:42.856005 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m16:46:42.856552 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/videos.py
[0m16:46:42.857089 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/comments.py
[0m16:46:43.185535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac68d3350>]}
[0m16:46:43.251051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac6a9efd0>]}
[0m16:46:43.251547 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:46:43.251883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac6827290>]}
[0m16:46:43.253265 [info ] [MainThread]: 
[0m16:46:43.253747 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:46:43.256760 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:46:43.257099 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:46:44.382596 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:46:44.383276 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:46:44.977600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac68d3610>]}
[0m16:46:44.980874 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:46:44.982925 [info ] [MainThread]: 
[0m16:46:44.993213 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:46:44.996118 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m16:46:44.998984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:46:45.001250 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:46:45.044891 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:46:45.045753 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:46:45.078172 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m16:46:45.079013 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import subprocess
import pandas as pd

import re
from html import unescape
import string

# from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # # Create a TextBlob object
    # blob = TextBlob(text)

    # # Return the sentiment score
    # return blob.sentiment.polarity
    return 0

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').toPandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m16:46:45.718682 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 59c997f5-efdd-45b1-bf81-385f7ff25dfd
[0m16:49:14.427710 [debug] [Thread-1 (]: Execution status: OK in 149.35000610351562 seconds
[0m16:49:14.473181 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac66dd710>]}
[0m16:49:14.474171 [info ] [Thread-1 (]: 1 of 3 OK created python table model youtube_sentiment.comments ................ [[32mOK[0m in 149.47s]
[0m16:49:14.475092 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:49:14.475776 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:49:14.476709 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:49:14.477408 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:49:14.477931 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:49:14.481230 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:49:14.482018 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:49:14.484393 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:49:15.116990 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:49:15.118480 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:49:15.533171 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:f0ac451a-c541-4214-902d-5f6681663877&page=queryresults
[0m16:49:18.054982 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac6735bd0>]}
[0m16:49:18.056255 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.58s]
[0m16:49:18.057718 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:49:18.058664 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:49:18.060130 [info ] [Thread-1 (]: 3 of 3 START python table model youtube_sentiment.videos ....................... [RUN]
[0m16:49:18.061133 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.users, now model.yt_comment_analysis.videos)
[0m16:49:18.061957 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.videos
[0m16:49:18.068635 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.videos"
[0m16:49:18.069373 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.videos
[0m16:49:18.073394 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.videos"
[0m16:49:18.074626 [debug] [Thread-1 (]: On model.yt_comment_analysis.videos: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd

import re

# Transform DURATION
def iso8601_to_minutes(duration: str) -> int:
    # Use regex to find hours, minutes, and seconds in the duration string
    hours = re.search(r'(\d+)H', duration)
    minutes = re.search(r'(\d+)M', duration)
    seconds = re.search(r'(\d+)S', duration)
    
    # Convert found values to integers, default to 0 if not found
    hours = int(hours.group(1)) if hours else 0
    minutes = int(minutes.group(1)) if minutes else 0
    seconds = int(seconds.group(1)) if seconds else 0
    
    # Calculate total minutes
    total_minutes = hours * 60 + minutes + (seconds / 60)
    
    return int(total_minutes)

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["re"]
    )
    # Load raw_videos data
    raw_videos_df = dbt.source('youtube_sentiment', 'raw_videos').toPandas()

    # Transform data
    raw_videos_df['DURATION'] = raw_videos_df['DURATION'].apply(iso8601_to_minutes)
    raw_videos_df['CAPTION'] = raw_videos_df['CAPTION'] == 'true'

    # Load comments data for aggregation
    comments_df = dbt.ref("comments").to_pandas()

    # Calculate aggregate metrics
    video_agg = comments_df.groupby('VIDEO_ID').agg(
        AVERAGE_SCORE=('SCORE', 'mean'),
        MAX_SCORE=('SCORE', 'max'),
        MIN_SCORE=('SCORE', 'min')
    ).reset_index()

    # Merge aggregate metrics with raw_videos
    merged_df = raw_videos_df.merge(video_agg, left_on='ID', right_on='VIDEO_ID', how='left')

    return merged_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {"comments": "sentiment-analysis-410608.youtube_sentiment.comments"}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_videos": "sentiment-analysis-410608.youtube_sentiment.raw_videos"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "videos"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.videos'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.videos")

  
[0m16:49:18.853537 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 8ba4b9b3-d843-4b53-8ac3-6ef83f4f9c01
[0m16:51:27.062460 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/yt_comment_analysis/models/videos.py[0m
Job failed with message [AttributeError: 'DataFrame' object has no attribute 'to_pandas'. Did you mean: 'toPandas'?]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/8ba4b9b3-d843-4b53-8ac3-6ef83f4f9c01?project=sentiment-analysis-410608
gcloud dataproc batches wait '8ba4b9b3-d843-4b53-8ac3-6ef83f4f9c01' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/3339459a-d509-40a5-a920-68ffd367c4cc/jobs/srvls-batch-40080af7-89e3-464a-b065-5ae8073b548c/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/3339459a-d509-40a5-a920-68ffd367c4cc/jobs/srvls-batch-40080af7-89e3-464a-b065-5ae8073b548c/driveroutput.*
[0m16:51:27.078965 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 368, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 314, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/task/run.py", line 298, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 89, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/clients/jinja.py", line 84, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 298, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 768, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 55, in macro
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/sandbox.py", line 394, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/context/providers.py", line 1444, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 175, in execution_with_log
    response = code_execution_function(*args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1536, in submit_python_job
    submission_result = job_helper.submit(compiled_code)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 75, in submit
    return self._submit_dataproc_job()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/python_submissions.py", line 144, in _submit_dataproc_job
    return poll_batch_job(
           ^^^^^^^^^^^^^^^
  File "/home/micasidad/Desktop/yt-comment-analysis/venv/lib/python3.11/site-packages/dbt/adapters/bigquery/dataproc/batch.py", line 51, in poll_batch_job
    raise ValueError(response.state_message)
ValueError: Job failed with message [AttributeError: 'DataFrame' object has no attribute 'to_pandas'. Did you mean: 'toPandas'?]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/8ba4b9b3-d843-4b53-8ac3-6ef83f4f9c01?project=sentiment-analysis-410608
gcloud dataproc batches wait '8ba4b9b3-d843-4b53-8ac3-6ef83f4f9c01' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/3339459a-d509-40a5-a920-68ffd367c4cc/jobs/srvls-batch-40080af7-89e3-464a-b065-5ae8073b548c/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/3339459a-d509-40a5-a920-68ffd367c4cc/jobs/srvls-batch-40080af7-89e3-464a-b065-5ae8073b548c/driveroutput.*

[0m16:51:27.086066 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89250fee-dab1-463b-8c9d-4f162acadd7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ac671bc50>]}
[0m16:51:27.088954 [error] [Thread-1 (]: 3 of 3 ERROR creating python table model youtube_sentiment.videos .............. [[31mERROR[0m in 129.02s]
[0m16:51:27.091570 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:51:27.096451 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:51:27.097884 [debug] [MainThread]: Connection 'model.yt_comment_analysis.videos' was properly closed.
[0m16:51:27.099228 [info ] [MainThread]: 
[0m16:51:27.100344 [info ] [MainThread]: Finished running 3 table models in 0 hours 4 minutes and 43.85 seconds (283.85s).
[0m16:51:27.104081 [debug] [MainThread]: Command end result
[0m16:51:27.142065 [info ] [MainThread]: 
[0m16:51:27.142540 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:51:27.142939 [info ] [MainThread]: 
[0m16:51:27.143433 [error] [MainThread]:   Job failed with message [AttributeError: 'DataFrame' object has no attribute 'to_pandas'. Did you mean: 'toPandas'?]. Additional details can be found at:
https://console.cloud.google.com/dataproc/batches/australia-southeast1/8ba4b9b3-d843-4b53-8ac3-6ef83f4f9c01?project=sentiment-analysis-410608
gcloud dataproc batches wait '8ba4b9b3-d843-4b53-8ac3-6ef83f4f9c01' --region 'australia-southeast1' --project 'sentiment-analysis-410608'
https://console.cloud.google.com/storage/browser/dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/3339459a-d509-40a5-a920-68ffd367c4cc/jobs/srvls-batch-40080af7-89e3-464a-b065-5ae8073b548c/
gs://dataproc-staging-au-southeast1-906804028770-nrmedxw8/google-cloud-dataproc-metainfo/3339459a-d509-40a5-a920-68ffd367c4cc/jobs/srvls-batch-40080af7-89e3-464a-b065-5ae8073b548c/driveroutput.*
[0m16:51:27.143932 [info ] [MainThread]: 
[0m16:51:27.144514 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m16:51:27.145446 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 285.18216, "process_user_time": 4.557587, "process_kernel_time": 0.675499, "process_mem_max_rss": "216388", "process_out_blocks": "3024", "command_success": false, "process_in_blocks": "0"}
[0m16:51:27.146282 [debug] [MainThread]: Command `dbt run` failed at 16:51:27.145846 after 285.18 seconds
[0m16:51:27.146663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5aea6e1990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5aeccf6ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ae92d2d50>]}
[0m16:51:27.147037 [debug] [MainThread]: Flushing usage events
[0m16:52:46.427600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0e7f73550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0e7f73b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0e7f73850>]}


============================== 16:52:46.429637 | d97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0 ==============================
[0m16:52:46.429637 [info ] [MainThread]: Running with dbt=1.8.5
[0m16:52:46.430200 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:52:47.076509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c5807910>]}
[0m16:52:47.113639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0cc74f950>]}
[0m16:52:47.114164 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m16:52:47.121318 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m16:52:47.200664 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:52:47.201206 [debug] [MainThread]: Partial parsing: updated file: yt_comment_analysis://models/videos.py
[0m16:52:47.484419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c5780190>]}
[0m16:52:47.541223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c54d7b10>]}
[0m16:52:47.541682 [info ] [MainThread]: Found 3 models, 2 sources, 473 macros
[0m16:52:47.542062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c5590f10>]}
[0m16:52:47.543399 [info ] [MainThread]: 
[0m16:52:47.543994 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:52:47.546949 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m16:52:47.547350 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:52:48.645373 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m16:52:48.647339 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:52:49.259124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c57682d0>]}
[0m16:52:49.259885 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:52:49.260432 [info ] [MainThread]: 
[0m16:52:49.262790 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.comments
[0m16:52:49.263413 [info ] [Thread-1 (]: 1 of 3 START python table model youtube_sentiment.comments ..................... [RUN]
[0m16:52:49.264009 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.comments)
[0m16:52:49.264478 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.comments
[0m16:52:49.307804 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.comments"
[0m16:52:49.308447 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.comments
[0m16:52:49.320665 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:52:50.026307 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.comments"
[0m16:52:50.027232 [debug] [Thread-1 (]: On model.yt_comment_analysis.comments: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import sys
import os
import subprocess
import pandas as pd

import re
from html import unescape
import string

# from textblob import TextBlob


def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Unescape HTML entities (e.g., &amp; to &)
    text = unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_sentiment_score(text):
    # # Create a TextBlob object
    # blob = TextBlob(text)

    # # Return the sentiment score
    # return blob.sentiment.polarity
    return 0

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["re", "html"]
    )
    # Load raw_comments data
    raw_comments_df = dbt.source('youtube_sentiment', 'raw_comments').toPandas()

    # Transform data
    raw_comments_df['TEXT'] = raw_comments_df['TEXT'].apply(clean_text)
    raw_comments_df['SCORE'] = raw_comments_df['TEXT'].apply(get_sentiment_score)

    return raw_comments_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_comments": "sentiment-analysis-410608.youtube_sentiment.raw_comments"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "comments"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.comments'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.comments")

  
[0m16:52:50.676234 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 74413221-f734-416e-9010-7505bfb58137
[0m16:55:14.067519 [debug] [Thread-1 (]: Execution status: OK in 144.0399932861328 seconds
[0m16:55:14.089434 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c58d4f50>]}
[0m16:55:14.090172 [info ] [Thread-1 (]: 1 of 3 OK created python table model youtube_sentiment.comments ................ [[32mOK[0m in 144.82s]
[0m16:55:14.090893 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.comments
[0m16:55:14.091668 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m16:55:14.092330 [info ] [Thread-1 (]: 2 of 3 START sql table model youtube_sentiment.users ........................... [RUN]
[0m16:55:14.092844 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.comments, now model.yt_comment_analysis.users)
[0m16:55:14.093247 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m16:55:14.095641 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m16:55:14.096217 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m16:55:14.098450 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:55:14.900289 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m16:55:14.902302 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`raw_comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m16:55:15.585526 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:aaab8d0a-4397-4fa2-a647-d3db760804c6&page=queryresults
[0m16:55:17.557567 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c536b710>]}
[0m16:55:17.559986 [info ] [Thread-1 (]: 2 of 3 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (175.0 rows, 3.8 KiB processed)[0m in 3.46s]
[0m16:55:17.562584 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m16:55:17.564426 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.videos
[0m16:55:17.566723 [info ] [Thread-1 (]: 3 of 3 START python table model youtube_sentiment.videos ....................... [RUN]
[0m16:55:17.569247 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.users, now model.yt_comment_analysis.videos)
[0m16:55:17.571937 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.videos
[0m16:55:17.589064 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.videos"
[0m16:55:17.590794 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.videos
[0m16:55:17.596467 [debug] [Thread-1 (]: Writing runtime python for node "model.yt_comment_analysis.videos"
[0m16:55:17.597544 [debug] [Thread-1 (]: On model.yt_comment_analysis.videos: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","quang-ltu-bucket")
spark.conf.set("enableListInference", "True")


import pandas as pd

import re

# Transform DURATION
def iso8601_to_minutes(duration: str) -> int:
    # Use regex to find hours, minutes, and seconds in the duration string
    hours = re.search(r'(\d+)H', duration)
    minutes = re.search(r'(\d+)M', duration)
    seconds = re.search(r'(\d+)S', duration)
    
    # Convert found values to integers, default to 0 if not found
    hours = int(hours.group(1)) if hours else 0
    minutes = int(minutes.group(1)) if minutes else 0
    seconds = int(seconds.group(1)) if seconds else 0
    
    # Calculate total minutes
    total_minutes = hours * 60 + minutes + (seconds / 60)
    
    return int(total_minutes)

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["re"]
    )
    # Load raw_videos data
    raw_videos_df = dbt.source('youtube_sentiment', 'raw_videos').toPandas()

    # Transform data
    raw_videos_df['DURATION'] = raw_videos_df['DURATION'].apply(iso8601_to_minutes)
    raw_videos_df['CAPTION'] = raw_videos_df['CAPTION'] == 'true'

    # Load comments data for aggregation
    comments_df = dbt.ref("comments").toPandas()

    # Calculate aggregate metrics
    video_agg = comments_df.groupby('VIDEO_ID').agg(
        AVERAGE_SCORE=('SCORE', 'mean'),
        MAX_SCORE=('SCORE', 'max'),
        MIN_SCORE=('SCORE', 'min')
    ).reset_index()

    # Merge aggregate metrics with raw_videos
    merged_df = raw_videos_df.merge(video_agg, left_on='ID', right_on='VIDEO_ID', how='left')

    return merged_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {"comments": "sentiment-analysis-410608.youtube_sentiment.comments"}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {"youtube_sentiment.raw_videos": "sentiment-analysis-410608.youtube_sentiment.raw_videos"}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "sentiment-analysis-410608"
    schema = "youtube_sentiment"
    identifier = "videos"
    
    def __repr__(self):
        return 'sentiment-analysis-410608.youtube_sentiment.videos'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "indirect").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("sentiment-analysis-410608.youtube_sentiment.videos")

  
[0m16:55:18.235858 [info ] [Thread-1 (]: BigQuery adapter: Submitting batch job with id: 43231e50-c5a7-4f35-9cb1-56555b2fb3bd
[0m16:57:47.132636 [debug] [Thread-1 (]: Execution status: OK in 149.52999877929688 seconds
[0m16:57:47.138354 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97ffe34-5130-4fb7-b6ed-cb3e79e7b2c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0c572bdd0>]}
[0m16:57:47.139068 [info ] [Thread-1 (]: 3 of 3 OK created python table model youtube_sentiment.videos .................. [[32mOK[0m in 149.57s]
[0m16:57:47.139797 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.videos
[0m16:57:47.141163 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:57:47.141915 [debug] [MainThread]: Connection 'model.yt_comment_analysis.videos' was properly closed.
[0m16:57:47.142744 [info ] [MainThread]: 
[0m16:57:47.143506 [info ] [MainThread]: Finished running 3 table models in 0 hours 4 minutes and 59.60 seconds (299.60s).
[0m16:57:47.144740 [debug] [MainThread]: Command end result
[0m16:57:47.174353 [info ] [MainThread]: 
[0m16:57:47.175015 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:57:47.175386 [info ] [MainThread]: 
[0m16:57:47.175898 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m16:57:47.176632 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 300.78613, "process_user_time": 4.424541, "process_kernel_time": 0.670763, "process_mem_max_rss": "215772", "process_out_blocks": "3008", "process_in_blocks": "0"}
[0m16:57:47.177093 [debug] [MainThread]: Command `dbt run` succeeded at 16:57:47.177011 after 300.79 seconds
[0m16:57:47.177462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0eb982e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0e7fa53d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c0eb8a9250>]}
[0m16:57:47.177835 [debug] [MainThread]: Flushing usage events
[0m12:39:09.426929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a36013290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a36012f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a36012c90>]}


============================== 12:39:09.429590 | 0c73486c-b6e4-4f75-ad17-786c00208031 ==============================
[0m12:39:09.429590 [info ] [MainThread]: Running with dbt=1.8.5
[0m12:39:09.430138 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m12:39:10.392335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a12701b10>]}
[0m12:39:10.435540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a37b64f90>]}
[0m12:39:10.436321 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m12:39:10.456459 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m12:39:10.526011 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:39:10.526707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a37333a50>]}
[0m12:39:11.513060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a12e81b10>]}
[0m12:39:11.588969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a12ea6650>]}
[0m12:39:11.589587 [info ] [MainThread]: Found 2 models, 2 sources, 473 macros
[0m12:39:11.590041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a12e9a190>]}
[0m12:39:11.591553 [info ] [MainThread]: 
[0m12:39:11.592188 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:39:11.595365 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m12:39:11.595941 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:39:12.693587 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m12:39:12.695807 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:13.375374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a125c7510>]}
[0m12:39:13.378246 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:39:13.380102 [info ] [MainThread]: 
[0m12:39:13.399210 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m12:39:13.401863 [info ] [Thread-1 (]: 1 of 2 START sql table model youtube_sentiment.users ........................... [RUN]
[0m12:39:13.404218 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.users)
[0m12:39:13.406191 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m12:39:13.430195 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m12:39:13.431431 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m12:39:13.468461 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m12:39:13.469100 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:39:13.470347 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m12:39:15.660834 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:56af725d-7e9f-40ef-be2e-900d1ab000cf&page=queryresults
[0m12:39:18.263370 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a3758ec90>]}
[0m12:39:18.264458 [info ] [Thread-1 (]: 1 of 2 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (6.1k rows, 125.3 KiB processed)[0m in 4.86s]
[0m12:39:18.265551 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m12:39:18.266302 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.video_scores
[0m12:39:18.267132 [info ] [Thread-1 (]: 2 of 2 START sql view model youtube_sentiment.video_scores ..................... [RUN]
[0m12:39:18.267849 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.users, now model.yt_comment_analysis.video_scores)
[0m12:39:18.268427 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.video_scores
[0m12:39:18.276266 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.video_scores"
[0m12:39:18.277241 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.video_scores
[0m12:39:18.297468 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.video_scores"
[0m12:39:18.298054 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:39:18.299328 [debug] [Thread-1 (]: On model.yt_comment_analysis.video_scores: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.video_scores"} */


  create or replace view `sentiment-analysis-410608`.`youtube_sentiment`.`video_scores`
  OPTIONS()
  as WITH comment_scores AS (
    SELECT
        VIDEO_ID,
        AVG(SCORE) AS AVERAGE_SCORE,
        MAX(SCORE) AS MAX_SCORE,
        MIN(SCORE) AS MIN_SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`comments` -- This refers to the comments table in dbt
    GROUP BY
        VIDEO_ID
)

SELECT
    v.ID,
    v.TITLE,
    v.DESCRIPTION,
    v.PUBLISHED_AT,
    v.CATEGORY_ID,
    v.DURATION,
    v.CAPTION,
    v.LIKE_COUNT,
    v.COMMENT_COUNT,
    v.VIEW_COUNT,
    cs.AVERAGE_SCORE,
    cs.MAX_SCORE,
    cs.MIN_SCORE
FROM
    `sentiment-analysis-410608`.`youtube_sentiment`.`videos` v -- This refers to the videos table in dbt
LEFT JOIN
    comment_scores cs
ON
    v.ID = cs.VIDEO_ID;


[0m12:39:19.040921 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:aaa32219-a80b-4d51-ba76-aeb8dc6b7e75&page=queryresults
[0m12:39:19.714457 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c73486c-b6e4-4f75-ad17-786c00208031', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a1255cc50>]}
[0m12:39:19.717264 [info ] [Thread-1 (]: 2 of 2 OK created sql view model youtube_sentiment.video_scores ................ [[32mCREATE VIEW (0 processed)[0m in 1.45s]
[0m12:39:19.720340 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.video_scores
[0m12:39:19.725585 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:39:19.727589 [debug] [MainThread]: Connection 'model.yt_comment_analysis.video_scores' was properly closed.
[0m12:39:19.729348 [info ] [MainThread]: 
[0m12:39:19.731227 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 8.14 seconds (8.14s).
[0m12:39:19.735137 [debug] [MainThread]: Command end result
[0m12:39:19.776142 [info ] [MainThread]: 
[0m12:39:19.776761 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:39:19.777180 [info ] [MainThread]: 
[0m12:39:19.777745 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m12:39:19.778503 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.390996, "process_user_time": 4.330287, "process_kernel_time": 0.298915, "process_mem_max_rss": "226348", "process_in_blocks": "44952", "process_out_blocks": "2896"}
[0m12:39:19.779004 [debug] [MainThread]: Command `dbt run` succeeded at 12:39:19.778914 after 10.39 seconds
[0m12:39:19.779422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a39983d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a398a67d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a5a39a27150>]}
[0m12:39:19.779778 [debug] [MainThread]: Flushing usage events
[0m12:39:54.739148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb11819790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1184ddd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1184dad0>]}


============================== 12:39:54.741371 | d64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0 ==============================
[0m12:39:54.741371 [info ] [MainThread]: Running with dbt=1.8.5
[0m12:39:54.742058 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/micasidad/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:39:55.611099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faee1ac3d0>]}
[0m12:39:55.648593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb135416d0>]}
[0m12:39:55.649158 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m12:39:55.656159 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m12:39:55.719587 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:39:55.720129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb12d079d0>]}
[0m12:39:56.611944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faef0e6c10>]}
[0m12:39:56.672895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faeea28e50>]}
[0m12:39:56.673383 [info ] [MainThread]: Found 2 models, 2 sources, 473 macros
[0m12:39:56.673831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faee056090>]}
[0m12:39:56.675346 [info ] [MainThread]: 
[0m12:39:56.675924 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:39:56.679234 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m12:39:56.679803 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:39:57.648102 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m12:39:57.650032 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:58.316964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faedccac10>]}
[0m12:39:58.319591 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:39:58.321348 [info ] [MainThread]: 
[0m12:39:58.330892 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m12:39:58.333360 [info ] [Thread-1 (]: 1 of 2 START sql table model youtube_sentiment.users ........................... [RUN]
[0m12:39:58.335738 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.users)
[0m12:39:58.337670 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m12:39:58.357126 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m12:39:58.358311 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m12:39:58.375613 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:39:58.989138 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m12:39:58.989968 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m12:40:00.282643 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:09db0128-0111-4984-b87b-26ca8d7a2161&page=queryresults
[0m12:40:02.642380 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faede1fbd0>]}
[0m12:40:02.643304 [info ] [Thread-1 (]: 1 of 2 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (6.1k rows, 125.3 KiB processed)[0m in 4.31s]
[0m12:40:02.644198 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m12:40:02.644883 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.video_scores
[0m12:40:02.645682 [info ] [Thread-1 (]: 2 of 2 START sql table model youtube_sentiment.video_scores .................... [RUN]
[0m12:40:02.646368 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.users, now model.yt_comment_analysis.video_scores)
[0m12:40:02.646947 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.video_scores
[0m12:40:02.652455 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.video_scores"
[0m12:40:02.653156 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.video_scores
[0m12:40:02.655912 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:40:03.487035 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.video_scores"
[0m12:40:03.489673 [debug] [Thread-1 (]: On model.yt_comment_analysis.video_scores: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.video_scores"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`video_scores`
      
    
    

    OPTIONS()
    as (
      WITH comment_scores AS (
    SELECT
        VIDEO_ID,
        AVG(SCORE) AS AVERAGE_SCORE,
        MAX(SCORE) AS MAX_SCORE,
        MIN(SCORE) AS MIN_SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`comments` -- This refers to the comments table in dbt
    GROUP BY
        VIDEO_ID
)

SELECT
    v.ID,
    v.TITLE,
    v.DESCRIPTION,
    v.PUBLISHED_AT,
    v.CATEGORY_ID,
    v.DURATION,
    v.CAPTION,
    v.LIKE_COUNT,
    v.COMMENT_COUNT,
    v.VIEW_COUNT,
    cs.AVERAGE_SCORE,
    cs.MAX_SCORE,
    cs.MIN_SCORE
FROM
    `sentiment-analysis-410608`.`youtube_sentiment`.`videos` v -- This refers to the videos table in dbt
LEFT JOIN
    comment_scores cs
ON
    v.ID = cs.VIDEO_ID
    );
  
[0m12:40:04.812883 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:e362dc4a-affd-49dc-b686-aec0e62b1566&page=queryresults
[0m12:40:07.090552 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd64a5ebe-3c34-4e7c-a70d-f4cb71dc1ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faedfafbd0>]}
[0m12:40:07.093256 [info ] [Thread-1 (]: 2 of 2 OK created sql table model youtube_sentiment.video_scores ............... [[32mCREATE TABLE (265.0 rows, 324.0 KiB processed)[0m in 4.44s]
[0m12:40:07.095839 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.video_scores
[0m12:40:07.100015 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:40:07.101635 [debug] [MainThread]: Connection 'model.yt_comment_analysis.video_scores' was properly closed.
[0m12:40:07.103487 [info ] [MainThread]: 
[0m12:40:07.105302 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 10.43 seconds (10.43s).
[0m12:40:07.108962 [debug] [MainThread]: Command end result
[0m12:40:07.153492 [info ] [MainThread]: 
[0m12:40:07.153953 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:40:07.154309 [info ] [MainThread]: 
[0m12:40:07.154697 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m12:40:07.155284 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.452245, "process_user_time": 4.106297, "process_kernel_time": 0.240665, "process_mem_max_rss": "226624", "process_out_blocks": "2896", "process_in_blocks": "0"}
[0m12:40:07.156054 [debug] [MainThread]: Command `dbt run` succeeded at 12:40:07.155923 after 12.45 seconds
[0m12:40:07.156585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1185a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb15363d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb152867d0>]}
[0m12:40:07.157017 [debug] [MainThread]: Flushing usage events
[0m14:02:30.501581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754989c4fad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754989c4f490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754989c4f1d0>]}


============================== 14:02:30.503976 | 033007bf-a102-4e45-9e47-e3a3a8eb2a74 ==============================
[0m14:02:30.503976 [info ] [MainThread]: Running with dbt=1.8.5
[0m14:02:30.504662 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/micasidad/Desktop/yt-comment-analysis/dbt/logs', 'version_check': 'True', 'profiles_dir': '/home/micasidad/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:02:31.424463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75496643af50>]}
[0m14:02:31.464320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75498b975910>]}
[0m14:02:31.464831 [info ] [MainThread]: Registered adapter: bigquery=1.8.2
[0m14:02:31.481336 [debug] [MainThread]: checksum: 643640a45b16e0d012867be4ac17daaa0c3ce97300faec3ce9a6de300b5e1c09, vars: {}, profile: , target: , version: 1.8.5
[0m14:02:31.578258 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:02:31.578643 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:02:31.602594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7549663e4850>]}
[0m14:02:31.667262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754966235e10>]}
[0m14:02:31.667805 [info ] [MainThread]: Found 2 models, 2 sources, 473 macros
[0m14:02:31.668185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7549663dad10>]}
[0m14:02:31.669621 [info ] [MainThread]: 
[0m14:02:31.670182 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:02:31.673201 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_sentiment-analysis-410608'
[0m14:02:31.673668 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:02:32.854715 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608, now list_sentiment-analysis-410608_youtube_sentiment)
[0m14:02:32.855299 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:02:33.554970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754966241610>]}
[0m14:02:33.557213 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:02:33.557753 [info ] [MainThread]: 
[0m14:02:33.570138 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.users
[0m14:02:33.571092 [info ] [Thread-1 (]: 1 of 2 START sql table model youtube_sentiment.users ........................... [RUN]
[0m14:02:33.571974 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_sentiment-analysis-410608_youtube_sentiment, now model.yt_comment_analysis.users)
[0m14:02:33.572679 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.users
[0m14:02:33.583630 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.users"
[0m14:02:33.584339 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.users
[0m14:02:33.600146 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:02:34.317134 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.users"
[0m14:02:34.317822 [debug] [Thread-1 (]: On model.yt_comment_analysis.users: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.users"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`users`
      
    
    

    OPTIONS()
    as (
      -- models/users.sql

WITH user_data AS (
    SELECT
        AUTHOR,
        COUNT(*) AS COMMENT_COUNT
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`comments`  -- Referencing the raw_comments table
    GROUP BY
        AUTHOR
)

SELECT * FROM user_data
    );
  
[0m14:02:36.031122 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:a5b0d8d7-f700-4285-90c0-55520ec4de2e&page=queryresults
[0m14:02:38.227803 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754965fc0c50>]}
[0m14:02:38.228739 [info ] [Thread-1 (]: 1 of 2 OK created sql table model youtube_sentiment.users ...................... [[32mCREATE TABLE (4.4k rows, 143.3 KiB processed)[0m in 4.66s]
[0m14:02:38.229635 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.users
[0m14:02:38.230288 [debug] [Thread-1 (]: Began running node model.yt_comment_analysis.video_scores
[0m14:02:38.231102 [info ] [Thread-1 (]: 2 of 2 START sql table model youtube_sentiment.video_scores .................... [RUN]
[0m14:02:38.231711 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.yt_comment_analysis.users, now model.yt_comment_analysis.video_scores)
[0m14:02:38.232205 [debug] [Thread-1 (]: Began compiling node model.yt_comment_analysis.video_scores
[0m14:02:38.236766 [debug] [Thread-1 (]: Writing injected SQL for node "model.yt_comment_analysis.video_scores"
[0m14:02:38.237768 [debug] [Thread-1 (]: Began executing node model.yt_comment_analysis.video_scores
[0m14:02:38.241500 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:02:38.900192 [debug] [Thread-1 (]: Writing runtime sql for node "model.yt_comment_analysis.video_scores"
[0m14:02:38.902803 [debug] [Thread-1 (]: On model.yt_comment_analysis.video_scores: /* {"app": "dbt", "dbt_version": "1.8.5", "profile_name": "yt_comment_analysis", "target_name": "dev", "node_id": "model.yt_comment_analysis.video_scores"} */

  
    

    create or replace table `sentiment-analysis-410608`.`youtube_sentiment`.`video_scores`
      
    
    

    OPTIONS()
    as (
      WITH comment_scores AS (
    SELECT
        VIDEO_ID,
        AVG(SCORE) AS AVERAGE_SCORE,
        MAX(SCORE) AS MAX_SCORE,
        MIN(SCORE) AS MIN_SCORE
    FROM
        `sentiment-analysis-410608`.`youtube_sentiment`.`comments` -- This refers to the comments table in dbt
    GROUP BY
        VIDEO_ID
)

SELECT
    v.ID,
    v.TITLE,
    v.DESCRIPTION,
    v.PUBLISHED_AT,
    v.CATEGORY_ID,
    v.DURATION,
    v.CAPTION,
    v.LIKE_COUNT,
    v.COMMENT_COUNT,
    v.VIEW_COUNT,
    cs.AVERAGE_SCORE,
    cs.MAX_SCORE,
    cs.MIN_SCORE
FROM
    `sentiment-analysis-410608`.`youtube_sentiment`.`videos` v -- This refers to the videos table in dbt
LEFT JOIN
    comment_scores cs
ON
    v.ID = cs.VIDEO_ID
    );
  
[0m14:02:40.639041 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=sentiment-analysis-410608&j=bq:australia-southeast1:27b353ac-ad4f-4615-ad40-00069a3922f2&page=queryresults
[0m14:02:43.419305 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '033007bf-a102-4e45-9e47-e3a3a8eb2a74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754966048f10>]}
[0m14:02:43.422037 [info ] [Thread-1 (]: 2 of 2 OK created sql table model youtube_sentiment.video_scores ............... [[32mCREATE TABLE (62.0 rows, 249.9 KiB processed)[0m in 5.19s]
[0m14:02:43.424715 [debug] [Thread-1 (]: Finished running node model.yt_comment_analysis.video_scores
[0m14:02:43.428989 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:02:43.430799 [debug] [MainThread]: Connection 'model.yt_comment_analysis.video_scores' was properly closed.
[0m14:02:43.432728 [info ] [MainThread]: 
[0m14:02:43.434113 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 11.76 seconds (11.76s).
[0m14:02:43.435090 [debug] [MainThread]: Command end result
[0m14:02:43.471153 [info ] [MainThread]: 
[0m14:02:43.472113 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:02:43.472824 [info ] [MainThread]: 
[0m14:02:43.473615 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:02:43.474789 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 13.01063, "process_user_time": 3.254793, "process_kernel_time": 0.284506, "process_mem_max_rss": "223744", "process_in_blocks": "46584", "process_out_blocks": "1944"}
[0m14:02:43.475943 [debug] [MainThread]: Command `dbt run` succeeded at 14:02:43.475772 after 13.01 seconds
[0m14:02:43.476826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75498d763d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x754989c835d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75498d686850>]}
[0m14:02:43.477694 [debug] [MainThread]: Flushing usage events
